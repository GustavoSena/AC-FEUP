{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To loan or not to loan - that is the question\n",
    "\n",
    "## Table Of Contents <a id=\"index\"></a>\n",
    "\n",
    "**Data Preparation**\n",
    "- [Functions Definition](#functions-definition)\n",
    "- [Import Dataset](#import-dataset)\n",
    "- [Loan Data](#loan-data)\n",
    "- [Districts Data](#districts-data)\n",
    "- [Disp Data](#disp-data)\n",
    "- [Client Data](#client-data)\n",
    "- [Transactions Data](#transactions-data)\n",
    "- [Card Data](#card-data)\n",
    "- [Data Exploration](#data-exploration)\n",
    "- [Dataset Managing](#dataset-managing)\n",
    "- [Clustering](#clustering)\n",
    "- [Experimental Setup](#experimental-setup)\n",
    "  \n",
    "**Models**\n",
    "- [Decision Tree](#decision-tree)\n",
    "    - [Parameter Tunning](#parameter-tunning)\n",
    "- [K-Nearest Neighbor](#k-nearest-neighbor)\n",
    "    - [Parameter Tunning](#parameter-tunning-2)  \n",
    "- [Support-Vector Machines](#support-vector-machines)\n",
    "    - [Parameter Tunning](#parameter-tunning-3)\n",
    "- [Neural Networks](#neural-networks)\n",
    "    - [Parameter Tunning](#parameter-tunning-4)\n",
    "- [Logistic Regression](#logistic-regression)\n",
    "    - [Parameter Tunning](#parameter-tunning-5)\n",
    "- [Naive Bayes](#naive-bayes)\n",
    "    - [Parameter Tunning](#parameter-tunning-6)\n",
    "- [Random Forest](#random-forest)\n",
    "    - [Parameter Tunning](#parameter-tunning-7)\n",
    "- [XGBoost](#xgboost)\n",
    "    - [Parameter Tunning](#parameter-tunning-8)\n",
    "- [Ada Boost](#ada-boost)\n",
    "    - [Parameter Tunning](#parameter-tunning-10)\n",
    "    \n",
    "**Analysis**    \n",
    "\n",
    "- [Results Analysis](#results-analysis)\n",
    "    \n",
    "**Predictions**\n",
    "- [Apply Model](#apply-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score, precision_score, silhouette_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, f_classif, chi2\n",
    "from datetime import date\n",
    "from sklearn import preprocessing\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from kneed import KneeLocator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_graphs = True\n",
    "run_aggr = False\n",
    "replaceStatus = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Functions Definition <a id=\"functions-definition\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(x, missing = \"\"):\n",
    "    return pd.read_csv('Dataset/' + x + '.csv', sep = ';', low_memory = False, na_values = missing_values).rename(str.strip, axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_min(x):\n",
    "    return x.abs().min()\n",
    "abs_min.__name__ = 'abs_min'\n",
    "\n",
    "def rangev(x):\n",
    "    return x.max() - x.min()\n",
    "rangev.__name__ = 'range'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decade(year):\n",
    "    year_int = int(year)\n",
    "    string = str(year_int//10) + \"0-\" + str(year_int//10) + \"9\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(year):\n",
    "    string = year[2:4] + \"/\" + year[0:2]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_parsed(year):\n",
    "    string = year[3:5] + \"/\" + year[0:2]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_balance(year):\n",
    "    year_int = int(float(year))\n",
    "    string = \"1\" + \"0\"*(len(str(year_int))-1) + \"-\" + \"9\" + \"9\"*(len(str(year_int))-1)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAge(birthDate, loanDate):\n",
    "    loan = list(map(int,loanDate.split(\"-\")))\n",
    "    birth = list(map(int,birthDate.split(\"-\")))\n",
    "\n",
    "    loan_date = date(loan[0], loan[1], loan[2])\n",
    "    \n",
    "\n",
    "    birth_date = date(birth[0], birth[1], birth[2])\n",
    "    return (int((loan_date-birth_date).days/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_credit_cash(val):\n",
    "    return sum(val==\"credit in cash\")\n",
    "def count_collect(val):\n",
    "    return sum(val==\"collection from another bank\")\n",
    "def count_with_cash(val):\n",
    "    return sum(val==\"withdrawal in cash\")\n",
    "def count_remi(val):\n",
    "    return sum(val==\"remittance to another bank\")\n",
    "def count_with_card(val):\n",
    "    return sum(val==\"credit card withdrawal\")\n",
    "def count_interest(val):\n",
    "    return sum(val==\"interest credited\")\n",
    "\n",
    "def count_withdrawal(val):\n",
    "    return sum(val==\"withdrawal\")\n",
    "def count_credit(val):\n",
    "    return sum(val==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the mean of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_credit_cash(val):\n",
    "    return np.mean(val==\"credit in cash\")\n",
    "def mean_collect(val):\n",
    "    return np.mean(val==\"collection from another bank\")\n",
    "def mean_with_cash(val):\n",
    "    return np.mean(val==\"withdrawal in cash\")\n",
    "def mean_remi(val):\n",
    "    return np.mean(val==\"remittance to another bank\")\n",
    "def mean_with_card(val):\n",
    "    return np.mean(val==\"credit card withdrawal\")\n",
    "def mean_interest(val):\n",
    "    return np.mean(val==\"interest credited\")\n",
    "\n",
    "def mean_withdrawal(val):\n",
    "    return np.mean(val==\"withdrawal\")\n",
    "def mean_credit(val):\n",
    "    return np.mean(val==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the standard deviation of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_credit_cash(val):\n",
    "    return np.std(val==\"credit in cash\")\n",
    "def std_collect(val):\n",
    "    return np.std(val==\"collection from another bank\")\n",
    "def std_with_cash(val):\n",
    "    return np.std(val==\"withdrawal in cash\")\n",
    "def std_remi(val):\n",
    "    return np.std(val==\"remittance to another bank\")\n",
    "def std_with_card(val):\n",
    "    return np.std(val==\"credit card withdrawal\")\n",
    "def std_interest(val):\n",
    "    return np.std(val==\"interest credited\")\n",
    "\n",
    "\n",
    "def std_withdrawal(val):\n",
    "    return np.std(val==\"withdrawal\")\n",
    "def std_credit(val):\n",
    "    return np.std(val==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the covariance of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_credit_cash(val):\n",
    "    return np.cov(val==\"credit in cash\")\n",
    "def cov_collect(val):\n",
    "    return np.cov(val==\"collection from another bank\")\n",
    "def cov_with_cash(val):\n",
    "    return np.cov(val==\"withdrawal in cash\")\n",
    "def cov_remi(val):\n",
    "    return np.cov(val==\"remittance to another bank\")\n",
    "def cov_with_card(val):\n",
    "    return np.cov(val==\"credit card withdrawal\")\n",
    "def cov_interest(val):\n",
    "    return np.cov(val==\"interest credited\")\n",
    "\n",
    "def cov_withdrawal(val):\n",
    "    return np.cov(val==\"withdrawal\")\n",
    "def cov_credit(val):\n",
    "    return np.cov(val==\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_disponents(val):\n",
    "    return sum(val==\"DISPONENT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_dataset(loan, district, client, card, trans, account, disp):\n",
    "\n",
    "    df = loan\n",
    "    df = pd.merge(df, trans, on = 'account_id', suffixes = ('', '_trans'))\n",
    "    df = pd.merge(df, account, on = 'account_id', suffixes = ('', '_account'))\n",
    "    \n",
    "    df = pd.merge(df, district.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "    df = pd.merge(df, disp, on = 'account_id', suffixes = ('', '_disp'))\n",
    "    df = pd.merge(df, card, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "    df = pd.merge(df, client, on = 'client_id', suffixes = ('', '_client'))\n",
    "    df = df.drop(['district_id_client'], axis=1)\n",
    "    df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to parse the dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(df, date_var, birth):\n",
    "    loan_dates = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in df[date_var]]\n",
    "    return [calculateAge(df[birth][n],loan_dates[n]) for n in range(0,len(df[birth]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates_no_conversion(df, date_var, birth):\n",
    "    loan_dates = df[date_var]\n",
    "    return [calculateAge(df[birth][n],loan_dates[n]) for n in range(0,len(df[birth]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDateAfter(trans, loan):\n",
    "\n",
    "    for loanI in loan.index:\n",
    "        loan_date = loan['date'][loanI]\n",
    "        transClient = trans[trans['account_id'] == loan['account_id'][loanI]]\n",
    "        # print(len(transClient))\n",
    "        for transactionI in transClient.index:\n",
    "            if transClient['date'][transactionI] > loan_date:\n",
    "                trans.drop(transactionI, axis = 0, inplace=True)\n",
    "    \n",
    "    return trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to parse date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_ranges(df,age):\n",
    "    age_loan_train = df[age].astype(str)\n",
    "    return [get_decade(age_loan_train[n]) for n in range(0, len(df))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to parse the client birth and genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_client(client_data):\n",
    "    birth_dates = client_data['birth_number']\n",
    "    dates_parsed = []\n",
    "    genre = []\n",
    "    for bdate in birth_dates:\n",
    "        month = int(str(bdate)[2:4])\n",
    "        if month > 12:\n",
    "            genre.append(0)\n",
    "            month = month - 50\n",
    "            if month < 10:\n",
    "                month = '0' + str(month)\n",
    "            else:\n",
    "                month = str(month)\n",
    "        else:\n",
    "            if month < 10:\n",
    "                month = '0' + str(month)\n",
    "            else:\n",
    "                month = str(month)\n",
    "            genre.append(1)\n",
    "        dates_parsed.append(str(bdate)[:2] + '-' + month + '-' + str(bdate)[4:])\n",
    "        \n",
    "    return dates_parsed, genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(train_data, test_data, option):\n",
    "    if option == 0:\n",
    "        train_data.drop(['operation_count_credit_cash', 'operation_count_collect', 'operation_count_with_cash', \n",
    "        'operation_count_remi', 'operation_count_with_card', 'operation_count_interest', 'operation_mean_credit_cash', \n",
    "        'operation_mean_collect', 'operation_mean_with_cash', 'operation_mean_remi', 'operation_mean_with_card', \n",
    "        'operation_mean_interest', 'operation_std_credit_cash', 'operation_std_collect', 'operation_std_with_cash', \n",
    "        'operation_std_remi', 'operation_std_with_card', 'operation_std_interest', 'operation_cov_credit_cash', \n",
    "        'operation_cov_collect', 'operation_cov_with_cash', 'operation_cov_remi', 'operation_cov_with_card', \n",
    "        'operation_cov_interest', 'balance_std', 'balance_last', 'type_count_withdrawal', 'type_count_credit', \n",
    "        'type_mean_withdrawal', 'type_mean_credit', 'type_std_withdrawal', 'type_std_credit', 'type_cov_withdrawal', 'type_cov_credit'], axis=1, inplace=True)\n",
    "        test_data.drop(['operation_count_credit_cash', 'operation_count_collect', 'operation_count_with_cash', \n",
    "        'operation_count_remi', 'operation_count_with_card', 'operation_count_interest', 'operation_mean_credit_cash', \n",
    "        'operation_mean_collect', 'operation_mean_with_cash', 'operation_mean_remi', 'operation_mean_with_card', \n",
    "        'operation_mean_interest', 'operation_std_credit_cash', 'operation_std_collect', 'operation_std_with_cash', \n",
    "        'operation_std_remi', 'operation_std_with_card', 'operation_std_interest', 'operation_cov_credit_cash', \n",
    "        'operation_cov_collect', 'operation_cov_with_cash', 'operation_cov_remi', 'operation_cov_with_card', \n",
    "        'operation_cov_interest', 'balance_std', 'balance_last', 'type_count_withdrawal', 'type_count_credit', \n",
    "        'type_mean_withdrawal', 'type_mean_credit', 'type_std_withdrawal', 'type_std_credit', 'type_cov_withdrawal', 'type_cov_credit'], axis=1, inplace=True)  \n",
    "    elif option == 1:\n",
    "        train_data.drop(['age_account_range', 'operation_count_credit_cash', 'operation_count_collect', \n",
    "        'operation_count_with_cash', 'operation_count_remi', 'operation_count_with_card', 'operation_count_interest', \n",
    "        'operation_mean_credit_cash', 'operation_mean_collect', 'operation_mean_with_cash', 'operation_mean_remi', \n",
    "        'operation_mean_with_card', 'operation_mean_interest', 'operation_std_credit_cash', 'operation_std_collect', \n",
    "        'operation_std_with_cash', 'operation_std_remi', 'operation_std_with_card', 'operation_std_interest', \n",
    "        'operation_cov_credit_cash', 'operation_cov_collect', 'operation_cov_with_cash', 'operation_cov_remi', \n",
    "        'operation_cov_with_card', 'operation_cov_interest', 'balance_std', 'balance_last', 'type_std_withdrawal', \n",
    "        'type_std_credit', 'type_cov_withdrawal', 'type_cov_credit', 'amount_std', 'amount_last', 'amount_range', \n",
    "        'balance_std', 'balance_last', 'balance_range'], axis=1, inplace=True)\n",
    "        test_data.drop(['age_account_range', 'operation_count_credit_cash', 'operation_count_collect', \n",
    "        'operation_count_with_cash', 'operation_count_remi', 'operation_count_with_card', 'operation_count_interest', \n",
    "        'operation_mean_credit_cash', 'operation_mean_collect', 'operation_mean_with_cash', 'operation_mean_remi', \n",
    "        'operation_mean_with_card', 'operation_mean_interest', 'operation_std_credit_cash', 'operation_std_collect', \n",
    "        'operation_std_with_cash', 'operation_std_remi', 'operation_std_with_card', 'operation_std_interest', \n",
    "        'operation_cov_credit_cash', 'operation_cov_collect', 'operation_cov_with_cash', 'operation_cov_remi', \n",
    "        'operation_cov_with_card', 'operation_cov_interest', 'balance_std', 'balance_last', 'type_std_withdrawal', \n",
    "        'type_std_credit', 'type_cov_withdrawal', 'type_cov_credit', 'amount_std', 'amount_last', 'amount_range', \n",
    "        'balance_std', 'balance_last', 'balance_range'], axis=1, inplace=True) \n",
    "    \n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_aggr_dataset_plots(loan, district, client, card, trans, account, disp):\n",
    "    # build aggregate dataset\n",
    "\n",
    "    # columns = trans.columns.values.tolist()\n",
    "    # columns.remove(\"trans_id\")\n",
    "    # columns.remove(\"amount\")\n",
    "    # columns.remove(\"balance\")\n",
    "    # columns.remove(\"operation\")\n",
    "    # columns.remove(\"type\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    trans=trans.groupby(\"account_id\", as_index=False,group_keys=False).agg({\n",
    "        \"operation\": [\"count\",count_credit_cash, count_collect, count_with_cash, count_remi, count_with_card, count_interest, \n",
    "                     mean_credit_cash, mean_collect, mean_with_cash, mean_remi, mean_with_card, mean_interest,\n",
    "                     std_credit_cash, std_collect, std_with_cash, std_remi, std_with_card, std_interest,\n",
    "                     cov_credit_cash, cov_collect, cov_with_cash, cov_remi, cov_with_card, cov_interest], \n",
    "        \"amount\": [\"mean\",\"min\",\"max\",\"std\",\"last\",abs_min,rangev],\n",
    "        \"balance\": [\"mean\",\"min\",\"max\",\"std\",\"last\",abs_min,rangev],\n",
    "        \"type\": [count_withdrawal, count_credit, mean_withdrawal, mean_credit, std_withdrawal, std_credit,cov_withdrawal, cov_credit]\n",
    "    })\n",
    "    trans.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in trans.columns]\n",
    "    \n",
    "    df = loan\n",
    "    df = pd.merge(df, trans, on = 'account_id', suffixes = ('', '_trans'))\n",
    "    df = pd.merge(df, account, on = 'account_id', suffixes = ('', '_account'))\n",
    "    df = pd.merge(df, district.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "    df = pd.merge(df, disp, on = 'account_id', suffixes = ('', '_disp'))\n",
    "    df = pd.merge(df, card, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "    df = pd.merge(df, client, on = 'client_id', suffixes = ('', '_client'))\n",
    "    df = df.drop(['district_id_client'], axis=1)\n",
    "\n",
    "\n",
    "    df.info()\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_aggr_dataset(loan, district, client, card, trans, account, disp):\n",
    "    # build aggregate dataset\n",
    "    \n",
    "    trans.loc[trans['operation'].isna(), 'operation'] = trans.loc[trans['operation'].isna(), 'k_symbol']\n",
    "    trans.loc[trans['type'] == \"withdrawal in cash\", 'type'] = \"withdrawal\"\n",
    "    trans.drop(['k_symbol', 'bank', 'account', 'date'], axis=1, inplace=True)\n",
    "    \n",
    "    trans_train['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in trans_train['date']]\n",
    "    loan_train['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_train['date']]\n",
    "    \n",
    "    trans_test['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in trans_test['date']]\n",
    "    loan_test['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_test['date']]\n",
    "    \n",
    "    trans_train = removeDateAfter(trans_train, loan_train)\n",
    "    trans_test = removeDateAfter(trans_test, loan_test)\n",
    "\n",
    "    trans=trans.groupby(\"account_id\", as_index=False,group_keys=False).agg({\n",
    "        \"operation\": [\"count\",count_credit_cash, count_collect, count_with_cash, count_remi, count_with_card, count_interest, \n",
    "                     mean_credit_cash, mean_collect, mean_with_cash, mean_remi, mean_with_card, mean_interest,\n",
    "                     std_credit_cash, std_collect, std_with_cash, std_remi, std_with_card, std_interest,\n",
    "                     cov_credit_cash, cov_collect, cov_with_cash, cov_remi, cov_with_card, cov_interest], \n",
    "        \"amount\": [\"mean\",\"min\",\"max\",\"std\",\"last\",abs_min,rangev],\n",
    "        \"balance\": [\"mean\",\"min\",\"max\",\"std\",\"last\",abs_min,rangev],\n",
    "        \"type\": [count_withdrawal, count_credit, mean_withdrawal, mean_credit, std_withdrawal, std_credit,cov_withdrawal, cov_credit]\n",
    "    })\n",
    "    trans.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in trans.columns]\n",
    "    \n",
    "\n",
    "    disponents = disp.groupby([\"account_id\"],as_index=False,group_keys=False).agg({\"type\": [count_disponents]})\n",
    "    disponents.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in disponents.columns]\n",
    "    disp = disp[disp.type.eq('OWNER')]\n",
    "    disp.drop(['type'], axis=1, inplace=True)\n",
    "    disp = pd.merge(disp,disponents, on = \"account_id\")\n",
    "\n",
    "    client[\"birth_number\"], client[\"genre\"] = parse_client(client)\n",
    "\n",
    "    district['unemploymant rate \\'95'].fillna(district['unemploymant rate \\'95'].median(), inplace=True)\n",
    "    district['no. of commited crimes \\'95'].fillna(district['no. of commited crimes \\'95'].mean(), inplace=True)\n",
    "    district.drop(['name', 'region'], axis=1, inplace=True)\n",
    "\n",
    "    df = loan\n",
    "    df = pd.merge(df, trans, on = 'account_id', suffixes = ('', '_trans'))\n",
    "    df = pd.merge(df, account, on = 'account_id', suffixes = ('', '_account'))\n",
    "    df = pd.merge(df, district.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "    df = pd.merge(df, disp, on = 'account_id', suffixes = ('', '_disp'))\n",
    "    df = pd.merge(df, card, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "    df = pd.merge(df, client, on = 'client_id', suffixes = ('', '_client'))\n",
    "    df = df.drop(['district_id_client'], axis=1)\n",
    "\n",
    "    \n",
    "\n",
    "    df[\"age_loan\"]=parse_dates_no_conversion(df,\"date\",\"birth_number\")\n",
    "    df[\"age_account\"]=parse_dates(df,\"date_account\",\"birth_number\")\n",
    "    df[\"age_loan_range\"] = parse_date_ranges(df,\"age_loan\")\n",
    "    df[\"age_account_range\"] = parse_date_ranges(df,\"age_account\")\n",
    "    df.drop(['date', 'date_account', 'birth_number'], axis=1, inplace=True)\n",
    "\n",
    "    df.drop(['disp_id', 'client_id'], axis=1, inplace=True)\n",
    "    \n",
    "    df[\"hasCard\"]=df[\"card_id\"].fillna(0)\n",
    "    df.loc[df[\"hasCard\"]!=0,\"hasCard\"] = 1\n",
    "    df[\"hasCard\"] = df[\"hasCard\"].astype(bool)\n",
    "    df.drop(['card_id', 'type', 'issued'], axis=1, inplace=True)\n",
    "    \n",
    "\n",
    "    df.info()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Import Dataset <a id=\"import-dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = ['?', 'NA', '']\n",
    "account_data = dataset('account', missing_values)\n",
    "client_data = dataset('client', missing_values)\n",
    "disp_data = dataset('disp', missing_values)\n",
    "district_data = dataset('district', missing_values)\n",
    "card_train = dataset('card_train', missing_values)\n",
    "card_test = dataset('card_test')\n",
    "loan_train = dataset('loan_train', missing_values)\n",
    "loan_test = dataset('loan_test')\n",
    "trans_train = dataset('trans_train', missing_values)\n",
    "trans_test = dataset('trans_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_aggr:\n",
    "    loan_train_aggr, district_data_train_aggr, client_data_train_aggr, card_train_aggr, trans_train_aggr, account_data_train_aggr, disp_data_train_aggr = loan_train.copy(), district_data.copy(), client_data.copy(), card_train.copy(), trans_train.copy(), account_data.copy(), disp_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_aggr:\n",
    "    loan_test_aggr, district_data_test_aggr, client_data_test_aggr, card_test_aggr, trans_test_aggr, account_data_test_aggr, disp_data_test_aggr = loan_test.copy(), district_data.copy(), client_data.copy(), card_test.copy(), trans_test.copy(), account_data.copy(), disp_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Loan Data <a id=\"loan-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we want to predict the probability of the *status* of a loan being equal to `-1`, and in the original data, `status = -1` represents loan not conceeded and `status = 1` represents loan conceeded, let's replace all `1` by `-1` and vice-versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if replaceStatus:\n",
    "    loan_train['status'] = loan_train['status'].replace(-1,0)\n",
    "    loan_train['status'] = loan_train['status'].replace(1,-1)\n",
    "    loan_train['status'] = loan_train['status'].replace(0,1)\n",
    "\n",
    "    loan_train.status.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at *status* values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    loan_train_copy = loan_train.copy()\n",
    "\n",
    "    ser = loan_train_copy.groupby('status')['status'].count()\n",
    "    ser = ser.sort_values(ascending=False)\n",
    "    ser = ser.iloc[[0,1]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.pie(ser.values, labels=['Successful', 'Unsuccessful'], startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "    ax.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.title('Loan Status Distribution', y=1.05, fontsize=20)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    plt.savefig(\"graphics/loan_status_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dataset is imbalanced, since we have too many examples of successful loans and only a few examples of unsuccessful loans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*amount* is the amount of money that the client borrowed, *duration* is the duration of the loan in months and *payments* is the amount of each monthly payment. Let's check if the *amount* value is equivalent to *duration* * *payments*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount = [loan_train['duration'][x] * loan_train['payments'][x] for x in loan_train.index]\n",
    "\n",
    "if amount == loan_train['amount'].tolist():\n",
    "    print (\"The lists are identical\")\n",
    "else :\n",
    "    print (\"The lists are not identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we can conclude that amount value is equivalent to duration * payments and when we finish the dataset analysis, we can drop *amount* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    loan_copy = loan_train.copy()\n",
    "    loan_copy = loan_copy.astype({'status': str})\n",
    "    loan_copy['status'] = loan_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "    loan_copy = loan_copy.groupby(['duration', 'status'])['status'].count().unstack().fillna(0)\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "    loan_copy.plot(kind='bar', stacked=True)\n",
    "    plt.title(\"Loan duration and loan success\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.savefig(\"graphics/duration_success.png\")        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.scatter(loan_train['duration'], loan_train['status'])\n",
    "    plt.title(\"Correlation between loan duration and loan success\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('status');\n",
    "    plt.xlabel('duration');\n",
    "    plt.savefig(\"graphics/corr_duration_success.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dur_st = np.corrcoef(loan_train['duration'], loan_train['status'])\n",
    "print(corr_dur_st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, loan *duration* and *status* are completely independent variables, since the correlation between them is near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.scatter(loan_train['duration'], loan_train['amount'])\n",
    "    plt.title(\"Correlation between loan duration and loan amount\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('amount');\n",
    "    plt.xlabel('duration');\n",
    "    plt.savefig(\"graphics/corr_duration_amount.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dur_amt = np.corrcoef(loan_train['duration'], loan_train['amount'])\n",
    "print(corr_dur_amt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Districts Data <a id=\"districts-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at null values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    # Null values for each attribute\n",
    "    district_data.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "    plt.title(\"Number of null variables per collumn\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('Null values');\n",
    "    plt.savefig(\"graphics/null_values_districts.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    district_scatter_plot = sb.PairGrid(district_data)\n",
    "    district_scatter_plot.map(plt.scatter)\n",
    "    plt.savefig(\"graphics/district_scatter_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.scatter(district_data['unemploymant rate \\'95'], district_data['unemploymant rate \\'96'])\n",
    "    plt.title(\"Correlation between district unemployment rate in \\'95 and \\'96\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('unemploymant rate \\'96');\n",
    "    plt.xlabel('unemploymant rate \\'95');\n",
    "    plt.savefig(\"graphics/corr_unemployment_95_96.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_un_95_96 = np.corrcoef(district_data['unemploymant rate \\'95'], district_data['unemploymant rate \\'96'])\n",
    "print(corr_un_95_96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at attribute values distribution for each of the columns that contain null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title('Distribution of district\\'s unemploymant rate in \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.hist(district_data['unemploymant rate \\'95'])\n",
    "    plt.savefig(\"graphics/districts_unemployment_95.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.title('Distribution of district\\'s no. of commited crimes \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.hist(district_data['no. of commited crimes \\'95'])\n",
    "    plt.savefig(\"graphics/districts_commited_crimes_95.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fill null values in in district's *unemploymant rate in '95* and *district's no. of commited crimes '95* with median and mean values of the column, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    district_data['unemploymant rate \\'95'].fillna(district_data['unemploymant rate \\'95'].median(), inplace=True)\n",
    "\n",
    "    district_data['no. of commited crimes \\'95'].fillna(district_data['no. of commited crimes \\'95'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sb.countplot(x='region', data=district_data)\n",
    "    plt.title(\"Number of districts per region\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.xlabel('Region');\n",
    "    plt.ylabel('Number of districts from region');\n",
    "    plt.savefig(\"graphics/districts_region.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study if the region might have an impact on the acceptance of the loan we will create a graph comparing the percentages of successful loans per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    district_data_region = district_data.copy()\n",
    "    region_data = loan_train\n",
    "    region_data = pd.merge(region_data, trans_train, on = 'account_id', suffixes = ('', '_trans'))\n",
    "    region_data = pd.merge(region_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "    #train_data = train_data.dropna()\n",
    "    region_data = pd.merge(region_data, district_data_region.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "\n",
    "    region_total = region_data[\"region\"].value_counts()\n",
    "    tuples_total = [tuple((x, y)) for x, y in region_total.items()]\n",
    "\n",
    "    regions_status_1 = region_data.loc[region_data['status'] == -1]\n",
    "    region_total_1 = regions_status_1[\"region\"].value_counts()\n",
    "    tuples_total_1 = [tuple((x, y)) for x, y in region_total_1.items()]\n",
    "\n",
    "    lista=[]\n",
    "    for x in tuples_total:\n",
    "        for y in tuples_total_1:\n",
    "            if x[0]==y[0]:\n",
    "                lista.append((x[0],x[1],y[1]))\n",
    "\n",
    "    percentages = [(i[0],i[2] / i[1]) for i in lista]\n",
    "    percentages.sort(key = lambda x: -x[1])\n",
    "\n",
    "    x = [i[0] for i in percentages]\n",
    "    y = [i[1] for i in percentages]\n",
    "\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sb.barplot(x,y)\n",
    "    plt.ylim(0.7, 1)\n",
    "    plt.title(\"Percentage of successful loans per region\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.xlabel('Region');\n",
    "    plt.ylabel('Percentage of successful loans');\n",
    "    plt.savefig(\"graphics/successful_loans_region.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    # 3D plot\n",
    "    fig = plt.figure(figsize=(8,8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_title(\"Number of municipalities with number of inhabitants in certain ranges\", fontdict={\"fontsize\":14, \"fontweight\": \"bold\"})\n",
    "    ax.set_xlabel(\"< 499 inhabitants\")\n",
    "    ax.set_ylabel(\"500-1999 inhabitants\")\n",
    "    ax.set_zlabel(\"2000-9999 inhabitants\")\n",
    "    ax.scatter(district_data[\"no. of municipalities with inhabitants < 499\"], district_data[\"no. of municipalities with inhabitants 500-1999\"], district_data[\"no. of municipalities with inhabitants 2000-9999\"])\n",
    "    plt.savefig(\"graphics/3d_district_municipalities.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *name* and the *region* of a **district** is not relevant to our analysis, as the *code* parameter is enough to identify a district. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    # Removing name and region from district\n",
    "    district_data.drop(['name', 'region'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Disp Data <a id=\"disp-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the different values the parameter type can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    disp_data_pie = disp_data.copy()\n",
    "\n",
    "    ser = disp_data_pie.groupby('type')['type'].count()\n",
    "    ser = ser.sort_values(ascending=False)\n",
    "    ser = ser.iloc[[0,1]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.pie(ser.values, labels=ser.index, startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "    ax.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.title('Type of disposition', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    fig.set_size_inches(15, 5)\n",
    "    plt.savefig(\"graphics/type_disposition.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only *OWNER* can issue permanent orders and ask for a loan, there is no interess in keeping this parameter in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    # only owner can issue permanent orders and ask for a loan\n",
    "    disponents = disp_data.groupby([\"account_id\"],as_index=False,group_keys=False).agg({\"type\": [count_disponents]})\n",
    "    disponents.columns = ['%s%s' % (a, '_%s' % b if b else '') for a, b in disponents.columns]\n",
    "    disp_owners = disp_data[disp_data.type.eq('OWNER')]\n",
    "    disp_owners.drop(['type'], axis=1, inplace=True)\n",
    "    disp_owners = pd.merge(disp_owners, disponents, on = \"account_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Client Data <a id=\"client-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *birth_number* column doesn't add any value to our model as it is in the original data (it's represented as an int in format YYMMDD for men and YYMM+50DD for women), let's put it in format YY-MM-DD and create a new column called *genre*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    client_data[\"birth_number\"], client_data[\"genre\"] = parse_client(client_data)\n",
    "    client_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the *genre* distribution, where 1 means male and 0 female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    client_data_genre_copy = client_data.copy()\n",
    "    ser = client_data_genre_copy.groupby('genre')['genre'].count()\n",
    "    ser = ser.sort_values(ascending=False)\n",
    "    ser = ser.iloc[[0,1]]\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.pie(ser.values, labels=[\"Male\",\"Female\"], startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "    ax.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.title('Genre distribution', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    fig.set_size_inches(15, 5)\n",
    "    plt.savefig(\"graphics/genre_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Transactions Data <a id=\"transactions-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at null values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    # Null values for each attribute\n",
    "    trans_train.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "    plt.title(\"Number of null variables per collumn\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('Null values');\n",
    "    plt.savefig(\"graphics/null_values_transactions.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    print(trans_train.operation.value_counts())\n",
    "    print('Null values: ' + str(trans_train.operation.isnull().sum()))\n",
    "    print()\n",
    "    print(trans_train.k_symbol.value_counts())\n",
    "    print('Null values: ' + str(trans_train.k_symbol.isnull().sum()))\n",
    "    print()\n",
    "    print(trans_train.type.value_counts())\n",
    "    print('Null values: ' + str(trans_train.type.isnull().sum()))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    trans_data_copy = trans_train.copy()\n",
    "    trans_data_copy = trans_data_copy.loc[:, trans_data_copy.columns.intersection(['type','operation'])]\n",
    "    trans_data_copy[\"operation\"] = trans_data_copy[\"operation\"].fillna('NaN')\n",
    "    trans_data_copy.sort_values(\"operation\")\n",
    "    sb.displot(trans_data_copy,x='operation', y='type', aspect=3)\n",
    "    plt.xlabel('Operation');\n",
    "    plt.ylabel('Type');\n",
    "    plt.title(\"Comparison between Operation and Type\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.savefig(\"graphics/comparison_operation_type.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the rows with operation *credit in cash* and *collection from another bank* are of type credit. All the *withdrawal in cash*, *remittance to another bank* and *credit cash withdrawal* operations are of the type *withdraw* or *withdrawal with cash* (in the case of the homonymous operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for **type** column, let's convert all *withdrawal in cash* occurrences into *withdrawal*, as it is basically the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    trans_train.loc[trans_train['type'] == \"withdrawal in cash\", 'type'] = \"withdrawal\"\n",
    "    trans_test.loc[trans_test['type'] == \"withdrawal in cash\", 'type'] = \"withdrawal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    plt.figure(figsize=(15,8))\n",
    "    sb.countplot(x='type', data=trans_train)\n",
    "    plt.title(\"Type Frequency\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.xlabel('Type');\n",
    "    plt.ylabel('Frequency');\n",
    "    plt.savefig(\"graphics/type_frequency.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    trans_data_copy = trans_train.copy()\n",
    "    trans_data_copy = trans_data_copy.groupby(['k_symbol', 'operation'])['operation'].count()\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "    trans_data_copy.plot(kind='bar', stacked=True)\n",
    "    plt.xlabel('K_symbol');\n",
    "    plt.ylabel('Quantity');\n",
    "    plt.title(\"Number of operation types per k_symbol\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.savefig(\"graphics/number_operation_types_per_ksymbol.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    trans_data_copy = trans_train.copy()\n",
    "    trans_data_copy[\"operation\"]=trans_data_copy[\"operation\"].fillna(\"Not Available\")\n",
    "    trans_data_copy[\"k_symbol\"]=trans_data_copy[\"k_symbol\"].fillna(\"Not Available\")\n",
    "    trans_data_copy['k_symbol'] = trans_data_copy['k_symbol'].replace([' '],['Not Available'])\n",
    "    trans_data_copy['operation'] = trans_data_copy['operation'].replace([''],['Not Available'])\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    sb.catplot(x=\"operation\", hue=\"k_symbol\", kind=\"count\",\n",
    "                palette=\"pastel\", edgecolor=\".6\",\n",
    "                data=trans_data_copy, height=10, aspect=1.5)\n",
    "    plt.title('Relation between k_symbols and operation', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylim(0)\n",
    "    plt.rcParams['figure.figsize']=(20,10)\n",
    "    plt.savefig(\"graphics/relation_k_symbol_operation.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the great majority of the k_symbol types are directly related to a certain operation type.\n",
    "All rows that have **operation** column with null value have *interested credited* in *k_symbol* column. We can try to replace the null values of *operation* column with the value that is in *k_symbol* column and then delete *k_symbol* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    trans_train.loc[trans_train['operation'].isna(), 'operation'] = trans_train.loc[trans_train['operation'].isna(), 'k_symbol']\n",
    "    trans_test.loc[trans_test['operation'].isna(), 'operation'] = trans_test.loc[trans_test['operation'].isna(), 'k_symbol']\n",
    "\n",
    "    trans_train.drop(['k_symbol'], axis=1, inplace=True)\n",
    "    trans_test.drop(['k_symbol'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows we can see the months when the most transfers were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    trans_data_copy = trans_train.copy()\n",
    "    trans_data_copy = trans_data_copy.sort_values('date')\n",
    "    trans_data_copy = trans_data_copy.astype({'date': str})\n",
    "\n",
    "    trans_data_copy['date'] = trans_data_copy['date'].apply(lambda x: get_month(x[0:4]))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(25,20))\n",
    "    plt.title(\"Months with the most tranfers\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    ax = sb.countplot(x=\"date\", data=trans_data_copy)\n",
    "    plt.savefig(\"graphics/months_with_most_transfers.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now remove the transactions that occur before the client asks for a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "\n",
    "    trans_train['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in trans_train['date']]\n",
    "    loan_train['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_train['date']]\n",
    "    \n",
    "    trans_test['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in trans_test['date']]\n",
    "    loan_test['date'] = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_test['date']]\n",
    "    \n",
    "    trans_train = removeDateAfter(trans_train, loan_train)\n",
    "    trans_test = removeDateAfter(trans_test, loan_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove *bank* and *account* columns, as these columns only record the destination account and have too many null values. *date* column is not relevant to our analysis too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    trans_train.drop(['bank', 'account', 'date'], axis=1, inplace=True)\n",
    "    trans_test.drop(['bank', 'account', 'date'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Card Data <a id=\"card-data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at null values distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    # Null values for each attribute\n",
    "    district_data.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "    plt.title(\"Number of null variables per collumn\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylabel('Null values');\n",
    "    plt.savefig(\"graphics/null_values_card.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    card_data_copy = card_train.copy()\n",
    "\n",
    "    ser = card_data_copy.groupby('type')['type'].count()\n",
    "    ser = ser.sort_values(ascending=False)\n",
    "    ser = ser.iloc[[0,1,2]]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    ax.pie(ser.values, labels=ser.index, startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "    ax.legend()\n",
    "    plt.axis('equal')\n",
    "    plt.title('Card Type distribution', y=1.05, fontsize=20)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    plt.savefig(\"graphics/card_type_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Data Exploration <a class=\"anchor\" id=\"data-exploration\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loan_duration(train_data):\n",
    "    if show_graphs:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        ax = sb.barplot(train_data[\"duration\"], train_data[\"amount\"])\n",
    "        plt.xlabel('Duration');\n",
    "        plt.ylabel('Amount');\n",
    "        plt.title('Loan duration compared the amount', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/loan_duration.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_number_payments(train_data):\n",
    "    if show_graphs:\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "        ax = plt.plot(train_data[\"amount\"], train_data[\"payments\"], linestyle='none', marker='o', alpha=0.3)\n",
    "        plt.title('Number of payments per different amount', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.xlabel('Amount');\n",
    "        plt.ylabel('Payments');\n",
    "        plt.savefig(\"graphics/number_payments.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_amount(train_data):\n",
    "    if show_graphs:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        #ax = sb.barplot(train_data[\"amount_trans\"], train_data[\"balance\"])\n",
    "        plt.plot(train_data[\"balance_mean\"],train_data[\"amount_mean\"], 'o', color='green');\n",
    "        plt.title('Distribution of tranfer amount by balance', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.ylabel('Transfer Amount');\n",
    "        plt.xlabel('Balance');\n",
    "        plt.savefig(\"graphics/distribution_amount.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cities_success(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy = train_data_copy.astype({'status': str})\n",
    "        train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "        train_data_copy = train_data_copy.groupby(['no. of cities', 'status'])['status'].count().unstack().fillna(0)\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "        train_data_copy.plot(kind='bar', stacked=True)\n",
    "        plt.title(\"Number of cities per district and loan success rate\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/cities_success.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inhabitants_success(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "\n",
    "        train_data_copy = train_data_copy.astype({'status': str})\n",
    "        train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "\n",
    "        fig = plt.figure()\n",
    "        sb.violinplot(y=train_data_copy[\"no. of inhabitants\"], x=train_data_copy[\"status\"])\n",
    "        plt.title('Influence of number of inhabitants in loan success', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.ylim(0)\n",
    "        fig.set_size_inches(15, 15)\n",
    "        plt.savefig(\"graphics/inhabitants_success.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_decade(train_data):\n",
    "    if show_graphs:\n",
    "        percentage = []\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "        train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "        train_data_copy = train_data_copy.astype({'status': str})\n",
    "        train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "        train_data_copy = train_data_copy.groupby(['age_loan', 'status'])['status'].count().unstack().fillna(0)\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "        train_data_copy.plot(kind='bar', stacked=True)\n",
    "        plt.title(\"Loan success rate per decade of age\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/success_decade.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_genre(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy = train_data_copy.astype({'status': str})\n",
    "        train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "        train_data_copy['genre'] = train_data_copy['genre'].replace([0,1],['Female','Male'])\n",
    "        train_data_copy = train_data_copy.groupby(['genre', 'status'])['status'].count().unstack().fillna(0)\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "        train_data_copy.plot(kind='bar', stacked=True)\n",
    "        plt.title(\"Loan success rate per genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/success_genre.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amount_age_genre(train_data):\n",
    "    if show_graphs:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        train_data_names = train_data.copy()\n",
    "        train_data_names[\"genre\"] = train_data_names[\"genre\"].astype(str)\n",
    "        train_data_names[\"genre\"].replace({\"1\": \"Male\" , \"0\": \"Female\"}, inplace=True)\n",
    "        sb.scatterplot(train_data_names[\"amount\"],train_data_names[\"age_loan\"],train_data_names[\"genre\"], alpha=0.5, sizes=(10, 1000), hue=\"time\")\n",
    "        plt.xlabel(\"Amount\")\n",
    "        plt.ylabel(\"Age\")\n",
    "        plt.title(\"Amount by age and genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/amount_age_genre.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_amount_age(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_names = train_data.copy()\n",
    "        train_data_names = train_data_names.astype({'age_loan': str})\n",
    "        train_data_names['age_loan'] = train_data_names['age_loan'].apply(lambda x: get_decade(x))\n",
    "        train_data_names = train_data_names.sort_values('age_loan')\n",
    "        plt.figure(figsize=(15,10))\n",
    "        sb.boxplot(x=train_data_names[\"age_loan\"], y=train_data_names[\"amount\"])\n",
    "        plt.title(\"Loan amount by age\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/amount_age.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_date_amount_age(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_ages = train_data.copy()\n",
    "        train_data_ages = train_data_ages.astype({'age_loan': str})\n",
    "        train_data_ages['age_loan'] = train_data_ages['age_loan'].apply(lambda x: get_decade(x))\n",
    "        train_data_ages = train_data_ages.astype({'age_account': str})\n",
    "        train_data_ages['age_account'] = train_data_ages['age_account'].apply(lambda x: get_decade(x))\n",
    "        train_data_names = train_data_ages.sort_values('age_loan')\n",
    "        sb.barplot(x='age_account', y='amount', hue='age_loan', data=train_data_ages, saturation=0.8)\n",
    "        plt.title(\"Date of creation of account compared to amount and age when the loan was made\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/date_amount_age.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_balance_amount_age(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_ages = train_data.copy()\n",
    "        train_data_ages = train_data_ages.astype({'age_loan': str})\n",
    "        train_data_ages['age_loan'] = train_data_ages['age_loan'].apply(lambda x: get_decade(x))\n",
    "        train_data_balance = train_data_ages.astype({'balance_mean': str})\n",
    "        train_data_balance['balance'] = train_data_balance['balance_mean'].apply(lambda x: get_balance(x))\n",
    "        train_data_balance = train_data_balance.sort_values('balance_mean')\n",
    "        sb.boxplot(x='balance', y='amount', hue='age_loan', data=train_data_balance, saturation=0.8)\n",
    "        plt.title(\"Balance and amount of loan by age\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/balance_amount_age.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_salary_age_district(train_data):\n",
    "    if show_graphs:\n",
    "        plt.figure(figsize=(15,10))\n",
    "        train_data_names = train_data.copy()\n",
    "        train_data_names[\"genre\"] = train_data_names[\"genre\"].astype(str)\n",
    "        train_data_names[\"genre\"].replace({\"1\": \"Male\" , \"0\": \"Female\"}, inplace=True)\n",
    "        sb.scatterplot(train_data_names[\"average salary\"],train_data_names[\"age_loan\"],train_data_names[\"district_id\"], alpha=0.5, sizes=(10, 1000), hue=\"time\")\n",
    "        plt.xlabel(\"Average Salary\")\n",
    "        plt.ylabel(\"Age\")\n",
    "        plt.title(\"Average Salary by age and district\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/salary_age_district.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_decade(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "        train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "        train_data_copy = train_data_copy.sort_values('age_loan')\n",
    "        train_data_copy[\"average salary\"] = train_data_copy[\"average salary\"].astype(float)\n",
    "        fig = plt.figure()\n",
    "        sb.boxplot(x=train_data_copy[\"age_loan\"], y=train_data_copy[\"average salary\"])\n",
    "        plt.xlabel('age_loan', y=1.05, fontsize=15, labelpad=15)\n",
    "        plt.ylabel('average salary', x=0.7, fontsize=15, labelpad=15)\n",
    "        plt.title('Average salary per decade of age', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        fig.set_size_inches(15, 10)\n",
    "        plt.savefig(\"graphics/average_decade.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_genre(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy[\"genre\"] = train_data_copy[\"genre\"].astype(str)\n",
    "        train_data_copy[\"genre\"].replace({\"0\": \"Female\",\"1\": \"Male\"}, inplace=True)\n",
    "        plt.figure(figsize=(15,10))\n",
    "        plt.ylim(9000, 10000)\n",
    "        sb.barplot(x = train_data_copy[\"genre\"], y = train_data_copy[\"average salary\"])\n",
    "        plt.title(\"Average Salary by genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/average_genre.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_card_success(train_data):\n",
    "    if show_graphs:\n",
    "        train_data_copy = train_data.copy()\n",
    "        train_data_copy = train_data_copy.astype({'status': str})\n",
    "        train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "        train_data_copy['type'] = train_data_copy['type'].fillna('NaN')\n",
    "        train_data_copy = train_data_copy.groupby(['type', 'status'])['status'].count().unstack().fillna(0)\n",
    "        plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "        train_data_copy.plot(kind='bar', stacked=True)\n",
    "        plt.title(\"Card type and loan success rate\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.savefig(\"graphics/card_success.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_balance_status(train_data):\n",
    "    if show_graphs:\n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.plot(train_data[\"status\"],train_data[\"balance_mean\"], 'o', color='green');\n",
    "        plt.title('Relation between status and balance', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "        plt.ylabel('Balance');\n",
    "        plt.xlabel('Status');\n",
    "        plt.savefig(\"graphics/balance_status.png\")        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Dataset Managing <a id=\"dataset-managing\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    loan_train_plot, district_data_train_plot, client_data_train_plot, card_train_plot, trans_train_plot, account_data_train_plot, disp_data_train_plot = loan_train.copy(), district_data.copy(), client_data.copy(), card_train.copy(), trans_train.copy(), account_data.copy(), disp_owners.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    loan_test_plot, district_data_test_plot, client_data_test_plot, card_test_plot, trans_test_plot, account_data_test_plot, disp_data_test_plot = loan_test.copy(), district_data.copy(), client_data.copy(), card_test.copy(), trans_test.copy(), account_data.copy(), disp_owners.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the merge and aggregate function for both the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    train_data = merge_aggr_dataset_plots(loan_train_plot, district_data_train_plot, client_data_train_plot, card_train_plot, trans_train_plot, account_data_train_plot, disp_data_train_plot)\n",
    "else:\n",
    "    train_data = merge_aggr_dataset(loan_train_aggr, district_data_train_aggr, client_data_train_aggr, card_train_aggr, trans_train_aggr, account_data_train_aggr, disp_data_train_aggr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    test_data = merge_aggr_dataset_plots(loan_test_plot, district_data_test_plot, client_data_test_plot, card_test_plot, trans_test_plot, account_data_test_plot, disp_data_test_plot)\n",
    "else:\n",
    "    test_data = merge_aggr_dataset(loan_test_aggr, district_data_test_aggr, client_data_test_aggr, card_test_aggr, trans_test_aggr, account_data_test_aggr, disp_data_test_aggr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a column *hasCard* to identify if a client has a card or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    card_train[\"hasCard\"] = card_train[\"card_id\"].fillna(0)\n",
    "    card_train.loc[card_train[\"hasCard\"] != 0, \"hasCard\"] = 1\n",
    "    card_train[\"hasCard\"] = card_train[\"hasCard\"].astype(bool)\n",
    "    \n",
    "    card_test[\"hasCard\"] = card_test[\"card_id\"].fillna(0)\n",
    "    card_test.loc[card_test[\"hasCard\"] != 0, \"hasCard\"] = 1\n",
    "    card_test[\"hasCard\"] = card_test[\"hasCard\"].astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a column *age_loan* where we will keep the age of the client at the time of the requested loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    train_data[\"age_loan\"] = parse_dates_no_conversion(train_data, \"date\", \"birth_number\")\n",
    "    test_data[\"age_loan\"] = parse_dates_no_conversion(test_data, \"date\", \"birth_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's create a new column that represents the age of each client when he creates the account (*age_account*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    train_data[\"age_account\"] = parse_dates(train_data, \"date_account\", \"birth_number\")\n",
    "    test_data[\"age_account\"] = parse_dates(test_data, \"date_account\", \"birth_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's simplify *age_loan* and *age_account* values, converting them into age ranges divided by decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    train_data[\"age_loan_range\"] = parse_date_ranges(train_data, \"age_loan\")\n",
    "    test_data[\"age_loan_range\"] = parse_date_ranges(test_data, \"age_loan\")\n",
    "\n",
    "    train_data[\"age_account_range\"] = parse_date_ranges(train_data, \"age_account\")\n",
    "    test_data[\"age_account_range\"] = parse_date_ranges(test_data, \"age_account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data_copy.astype({'status': str})\n",
    "    train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "    fig = plt.figure()\n",
    "    sb.violinplot(y=train_data_copy[\"no. of inhabitants\"], x=train_data_copy[\"status\"])\n",
    "    plt.title('Influence of number of inhabitants in loan success', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    plt.ylim(0)\n",
    "    fig.set_size_inches(15, 15)\n",
    "    plt.savefig(\"graphics/number_inhabitants_success.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "    train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "    train_data_copy = train_data_copy.sort_values('age_loan')\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(\"Age by decade at the date of the loan\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    ax = sb.countplot(x=\"age_loan\", data=train_data_copy)\n",
    "    plt.savefig(\"graphics/decade_date_loan.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution when the loan is asked demonstrates an expected pattern: there are more loan requests when people are younger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data_copy.astype({'age_account': str})\n",
    "    train_data_copy['age_account'] = train_data_copy['age_account'].apply(lambda x: get_decade(x))\n",
    "    train_data_copy = train_data_copy.sort_values('age_account')\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(\"Age by decade at the date of the account_creation\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    ax = sb.countplot(x=\"age_account\", data=train_data_copy)\n",
    "    plt.savefig(\"graphics/decade_date_account.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the months when the accounts were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data_copy.sort_values('date_account')\n",
    "    train_data_copy = train_data_copy.astype({'date_account': str})\n",
    "    train_data_copy['date_account'] = train_data_copy['date_account'].apply(lambda x: get_month(x[0:4]))\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(25,20))\n",
    "    plt.title(\"Date of creation of the account per month\")\n",
    "    ax = sb.countplot(x=\"date_account\", data=train_data_copy)\n",
    "    plt.savefig(\"graphics/account_creation_month.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the month and year of when the loans were created and their relation to the number of successful loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    train_data_copy = train_data.copy()\n",
    "    train_data_copy = train_data_copy.sort_values('date')\n",
    "    train_data_copy = train_data_copy.astype({'date': str})\n",
    "    train_data_copy['date'] = train_data_copy['date'].apply(lambda x: get_month_parsed(x[0:5]))\n",
    "    train_data_copy = train_data_copy.astype({'status': str})\n",
    "    train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Unsuccessful Loan','Successful Loan'])\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(25,20))\n",
    "    plt.title(\"Number of successful loans per month\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "    ax = sb.countplot(x ='date', hue = \"status\", data = train_data_copy)\n",
    "    plt.savefig(\"graphics/success_month.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loan_duration(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that with bigger loan amounts the time taken to repay them is exponentially higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_number_payments(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can check by the values in the graph, it seems that the number of payments is defined by the bank since the results are very linear and follow a line according to the amount loaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distribution_amount(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see as the balance increases the transfer amount also increase and is rarely higher than the balance. It can also be seen that a straight continous line exists with gradient 1. This means the amount of some transfers are actually correspondent, or close, to the total amount in the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cities_success(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's no relation between the number of cities per district and the success of a loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inhabitants_success(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_success_decade(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_success_genre(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no relation between the *genre* of a person and the *status* of the respective loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_amount_age_genre(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no relation between *age*, *genre* and loan *amount*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_amount_age(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_decade(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last plots, we can see that there are some outliers in our data. However, when we try to remove them, the results when we apply the classification models were worse, so we opt to not remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_date_amount_age(train_data)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_salary_age_district(train_data)                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_genre(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_card_success(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, all clients that have a card have a successful loan. However, there are only 11 clients with card that have asked for a loan in our dataset. So, this is not a relevant attribute for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_balance_status(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no apparent relationship between *status* and *balance*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Clustering <a id=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMEANS Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_scores(x,y):\n",
    "    scaler = StandardScaler()\n",
    "    train_data_copy = train_data.copy()\n",
    "    scaled_features = scaler.fit_transform(train_data_copy[[x,y]])\n",
    "\n",
    "    kmeans_kwargs = {\n",
    "        \"init\": \"random\",\n",
    "        \"n_init\": 10,\n",
    "        \"max_iter\": 300,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    silhouette_coefficients = []\n",
    "\n",
    "    # Notice you start at 2 clusters for silhouette coefficient\n",
    "    for k in range(2, 11):\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        score = silhouette_score(scaled_features, kmeans.labels_)\n",
    "        silhouette_coefficients.append(score)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.plot(range(2, 11), silhouette_coefficients)\n",
    "    plt.xticks(range(2, 11))\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"Silhouette Coefficient\")\n",
    "    plt.savefig(\"graphics/silouette_scores_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    silhouette_scores('amount_mean','average salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elbow_scores(x,y):\n",
    "    scaler = StandardScaler()\n",
    "    train_data_copy = train_data.copy()\n",
    "    scaled_features = scaler.fit_transform(train_data_copy[[x,y]])\n",
    "\n",
    "    kmeans_kwargs = {\n",
    "        \"init\": \"random\",\n",
    "        \"n_init\": 10,\n",
    "        \"max_iter\": 300,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "\n",
    "    sse = []\n",
    "\n",
    "    for k in range(1, 11):\n",
    "        kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "        kmeans.fit(scaled_features)\n",
    "        sse.append(kmeans.inertia_)\n",
    "\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    plt.plot(range(1, 11), sse)\n",
    "    plt.xticks(range(1, 11))\n",
    "    plt.xlabel(\"Number of Clusters\")\n",
    "    plt.ylabel(\"SSE\")\n",
    "    plt.savefig(\"graphics/elbow_scores_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()\n",
    "    \n",
    "    kl = KneeLocator(\n",
    "        range(1, 11), sse, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "    print(\"The optimal number of clusters is \"+str(kl.elbow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    elbow_scores('amount_mean','average salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_2d(x,y, n, x_name, y_name):\n",
    "    train_data_copy = train_data.copy()\n",
    "    \n",
    "    # create kmeans object\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    # fit kmeans object to data\n",
    "    kmeans.fit(train_data_copy[[x,y]])\n",
    "    # save new clusters for chart\n",
    "    identified_clusters = kmeans.fit_predict(train_data_copy[[x,y]])\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    plt.scatter(train_data_copy[x],train_data_copy[y], c= kmeans.labels_.astype(float), s=50, alpha=0.5)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n",
    "    plt.xlabel(x_name);\n",
    "    plt.ylabel(y_name);\n",
    "    plt.title(\"kmeans\")\n",
    "    plt.savefig(\"graphics/kmeans_cluster_2d_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    kmeans_cluster_2d('amount_mean','average salary',3,'Amount','Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_distance(x,y):\n",
    "    neigh = NearestNeighbors(n_neighbors=2)\n",
    "    train_data_copy = train_data.copy()\n",
    "    nbrs = neigh.fit(train_data_copy[[x,y]])\n",
    "    distances, indices = nbrs.kneighbors(train_data_copy[[x,y]])\n",
    "\n",
    "    # Plotting K-distance Graph\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,1]\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(distances)\n",
    "    plt.title('K-distance Graph',fontsize=20)\n",
    "    plt.xlabel('Data Points sorted by distance',fontsize=14)\n",
    "    plt.ylabel('Epsilon',fontsize=14)\n",
    "    plt.savefig(\"graphics/k_distance_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find a significant number for epsilon using k distance algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    K_distance('amount_mean','average salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is around 500 in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBSCAN_cluster_2d(x,y, n, x_name, y_name):\n",
    "    train_data_copy = train_data.copy()\n",
    "    dbscan_opt=DBSCAN(eps=n,min_samples=6)\n",
    "    dbscan_opt.fit(train_data_copy[[x,y]])\n",
    "    \n",
    "    train_data_copy['DBSCAN_opt_labels']=dbscan_opt.labels_\n",
    "    train_data_copy['DBSCAN_opt_labels'].value_counts()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    colors=['purple','red','blue','green']\n",
    "    plt.scatter(train_data_copy[x],train_data_copy[y],c=train_data_copy['DBSCAN_opt_labels'],cmap=matplotlib.colors.ListedColormap(colors),s=15)\n",
    "    plt.title('DBSCAN Clustering',fontsize=20)\n",
    "    plt.xlabel(x_name,fontsize=14)\n",
    "    plt.ylabel(y_name,fontsize=14)\n",
    "    plt.savefig(\"graphics/DBSCAN_cluster_2d_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    DBSCAN_cluster_2d('amount_mean','average salary',500,'Amount','Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Agglomerative/Hieralchical Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dendograms(x,y):\n",
    "    train_data_copy = train_data.copy()\n",
    "    linked = linkage(train_data_copy[[x,y]], 'single')\n",
    "\n",
    "    dendrogram(linked,\n",
    "                orientation='top',\n",
    "                distance_sort='descending',\n",
    "                show_leaf_counts=True)\n",
    "    plt.title(\"Dendogram\")\n",
    "    plt.savefig(\"graphics/dendograms_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    dendograms('amount_mean','average salary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agglomerative_cluster_2d(x,y, n, x_name, y_name):\n",
    "    train_data_copy = train_data.copy()\n",
    "    agglomerative=AgglomerativeClustering(n_clusters=n, affinity='euclidean')\n",
    "    agglomerative.fit(train_data_copy[[x,y]])\n",
    "    \n",
    "    train_data_copy['HR_labels']=agglomerative.labels_\n",
    "\n",
    "    colors=['purple','red','blue','green']\n",
    "    \n",
    "    # Plotting resulting clusters\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.scatter(train_data_copy[x],train_data_copy[y],c=train_data_copy['HR_labels'],cmap=matplotlib.colors.ListedColormap(colors),s=15)\n",
    "    plt.title('Hierarchical Clustering',fontsize=20)\n",
    "    plt.xlabel(x_name,fontsize=14)\n",
    "    plt.ylabel(y_name,fontsize=14)\n",
    "    plt.savefig(\"graphics/agglomerative_cluster_2d_\" + x + \"_\" + y + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    agglomerative_cluster_2d('amount_mean','average salary',2,'Amount','Salary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kmeans 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_cluster_3d(x,y,z, n, x_name, y_name, z_name):    \n",
    "    train_data_copy = train_data.copy()\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_data_copy[[x,y,z]] = scaler.fit_transform(train_data_copy[[x,y,z]])\n",
    "    \n",
    "    # create kmeans object\n",
    "    kmeans = KMeans(n_clusters=n)\n",
    "    # fit kmeans object to data\n",
    "    clusts = kmeans.fit(train_data_copy[[x,y,z]])\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    #Plot the clusters obtained using k means\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    print(centroids)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=50)\n",
    "    scatter = ax.scatter(train_data_copy[x],train_data_copy[y], train_data_copy[z],\n",
    "                         c=kmeans.labels_.astype(float),s=20)\n",
    "\n",
    "\n",
    "    ax.set_title('K-Means Clustering')\n",
    "    ax.set_xlabel(x_name)\n",
    "    ax.set_ylabel(y_name)\n",
    "    ax.set_zlabel(z_name)\n",
    "    ax.legend()\n",
    "    plt.savefig(\"graphics/kmeans_cluster_3d_\" + x + \"_\" + y + \"_\" + z + \".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_graphs:\n",
    "    kmeans_cluster_3d('amount_mean','average salary','no. of inhabitants',3,\"Amount\",\"average salary\", \"no. of inhabitants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Experimental Setup <a id=\"experimental-setup\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken all informations that we need from all dates, we can remove these columns from our dataset. We can also remove *issued* and *type* columns as they have too many null values. As we have already seen, *amount* attribute is also to be removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not run_aggr:\n",
    "    train_data.drop(['date', 'date_account', 'birth_number', 'age_loan', 'age_account', 'issued', 'type', 'amount'], axis=1, inplace=True)\n",
    "    test_data.drop(['date', 'date_account', 'birth_number', 'age_loan', 'age_account', 'issued', 'type', 'amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = -1\n",
    "train_data, test_data = remove_columns(train_data, test_data, option)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some categorical attributes in our dataset that we have to transform in numerical, so we can then apply the classification models. S, let's create dummies for these variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=['frequency'], dtype=bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['frequency'], dtype=bool)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['age_loan_range'], dtype = bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['age_loan_range'], dtype = bool)\n",
    "\n",
    "if option != 1:\n",
    "    train_data = pd.get_dummies(train_data, columns=['age_account_range'], dtype = bool)\n",
    "    test_data = pd.get_dummies(test_data, columns=['age_account_range'], dtype = bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All ids in our dataset can now also be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ids\n",
    "\n",
    "if not run_aggr:\n",
    "    train_data_no_ids = train_data.drop(['account_id', 'district_id', 'disp_id', 'client_id', 'card_id'], axis=1)\n",
    "    test_data_no_ids = test_data.drop(['account_id', 'district_id', 'disp_id', 'client_id', 'card_id'], axis=1)\n",
    "else:\n",
    "    train_data_no_ids = train_data.drop(['account_id', 'district_id', 'amount'], axis=1)\n",
    "    test_data_no_ids = test_data.drop(['account_id', 'district_id', 'amount'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = train_data_no_ids.corr().abs()\n",
    "plt.figure(figsize = (20,6))\n",
    "sb.heatmap(corr_matrix, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_data_no_ids[train_data_no_ids.columns.drop(['loan_id', 'status'])]\n",
    "all_labels = train_data_no_ids['status'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureSelection(x, y, n_feat):\n",
    "    best_features = SelectKBest(score_func=f_classif, k=n_feat)\n",
    "    features_fit = best_features.fit(x, y)\n",
    "    \n",
    "    df_scores = pd.DataFrame(features_fit.scores_)\n",
    "    df_features = pd.DataFrame(x.columns)\n",
    "    \n",
    "    feature_scores = pd.concat([df_features, df_scores], axis=1)\n",
    "    feature_scores.columns = ['Attr', 'Score']\n",
    "    feature_scores = feature_scores.nlargest(n_feat, 'Score')\n",
    "    \n",
    "    selected_features = feature_scores['Attr'].values.tolist()\n",
    "    \n",
    "    print(feature_scores)\n",
    "    print(selected_features)\n",
    "    \n",
    "    return x[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = featureSelection(all_inputs, all_labels, 18)\n",
    "print()\n",
    "print()\n",
    "print(all_inputs.info())\n",
    "selected_columns = all_inputs.columns\n",
    "app = pd.Index(['status', 'loan_id'])\n",
    "selected_columns = selected_columns.append(app)\n",
    "test_data_no_ids = test_data_no_ids[selected_columns]\n",
    "print()\n",
    "print()\n",
    "test_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test dataset with 25% of the credit_data_subset\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "X_train.columns = X_train.columns.str.translate(\"\".maketrans({\"[\":\"{\", \"]\":\"}\",\"<\":\"^\"}))\n",
    "X_test.columns = X_test.columns.str.translate(\"\".maketrans({\"[\":\"{\", \"]\":\"}\",\"<\":\"^\"}))\n",
    "test_data_no_ids.colums = test_data_no_ids.columns.str.translate(\"\".maketrans({\"[\":\"{\", \"]\":\"}\",\"<\":\"^\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion = True\n",
    "run_param_tunning = True\n",
    "run_classifiers = False\n",
    "run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Decision Tree <a class=\"anchor\" id=\"decision-tree\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Feature Selection\n",
    "# decision_tree_classifier = RFECV(decision_tree_classifier, scoring='roc_auc')\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = decision_tree_classifier.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = decision_tree_classifier.predict(X_test) \n",
    "dtc_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_dt = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_dt = roc_auc_score(y_test, predictions_test)\n",
    "pre_dt = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_dt))\n",
    "print(f\"ROC: {roc_dt}\")\n",
    "print(f\"Precision: {pre_dt}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/dt_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                    'splitter': ['best', 'random'],\n",
    "                    'max_depth': range(1, 10),\n",
    "                    'max_features': range(1,16)}\n",
    "\n",
    "    dt_classifier = DecisionTreeClassifier(min_samples_leaf = 10)\n",
    "\n",
    "    dt_grid_search = GridSearchCV(dt_classifier, scoring=\"roc_auc\", cv=10, param_grid=parameter_grid)\n",
    "    dt_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(dt_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = dt_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_dt = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = dt_grid_search.predict(X_test) \n",
    "    dtc_tun_classification_report = classification_report(y_test, predictions_test, target_names=['not pay', 'pay'], output_dict=True)\n",
    "    f1_dt_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_dt_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_dt_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_dt_tun))\n",
    "    print(f\"ROC: {roc_dt_tun}\")\n",
    "    print(f\"Precision: {pre_dt_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_dt = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/dt_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## K-Nearest Neighbor <a class=\"anchor\" id=\"k-nearest-neighbor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = knn.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = knn.predict(X_test) \n",
    "knn_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_knn = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_knn = roc_auc_score(y_test, predictions_test)\n",
    "pre_knn = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_knn))\n",
    "print(f\"ROC: {roc_knn}\")\n",
    "print(f\"Precision: {pre_knn}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/knn_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "    parameter_grid = {'n_neighbors': [5, 10, 15, 20],\n",
    "                      'leaf_size': [10, 20, 30, 40, 50],\n",
    "                      'weights': ['uniform','distance'],\n",
    "                      'p':[1,2],\n",
    "                      'algorithm':['ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "    knn_grid_search = GridSearchCV(knn_classifier, parameter_grid, scoring=\"roc_auc\", n_jobs=-1, cv=10)\n",
    "    knn_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(knn_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = knn_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_knn = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = knn_grid_search.predict(X_test) \n",
    "    knn_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_knn_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_knn_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_knn_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_knn_tun))\n",
    "    print(f\"ROC: {roc_knn_tun}\")\n",
    "    print(f\"Precision: {pre_knn_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_knn = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/knn_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Support-Vector Machines <a class=\"anchor\" id=\"support-vector-machines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = svc.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svc = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_svc, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = svc.predict(X_test) \n",
    "svm_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_svm = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_svm = roc_auc_score(y_test, predictions_test)\n",
    "pre_svm = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_svm))\n",
    "print(f\"ROC: {roc_svm}\")\n",
    "print(f\"Precision: {pre_svm}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svc = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_svc, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/svm_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    svm_classifier = SVC(probability=True)\n",
    "\n",
    "#     param_grid = {'C': [0.1,1, 10, 100], \n",
    "#                       'gamma': [1,0.1,0.01,0.001],\n",
    "#                       'kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "    \n",
    "    param_grid = {'C': [0.1, 1, 10, 100, 1000],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']}\n",
    "\n",
    "    svm_grid_search = GridSearchCV(svm_classifier, scoring=\"roc_auc\", cv=5, param_grid=param_grid, refit=True)\n",
    "    svm_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(svm_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = svm_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_svm = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = svm_grid_search.predict(X_test) \n",
    "    svm_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_svm_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_svm_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_svm_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_svm_tun))\n",
    "    print(f\"ROC: {roc_svm_tun}\")\n",
    "    print(f\"Precision: {pre_svm_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_svm = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/svm_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Neural Networks <a class=\"anchor\" id=\"neural-networks\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Feature Selection\n",
    "#scaler = RFECV(scaler, scoring='roc_auc')\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train_nn = scaler.transform(X_train)\n",
    "X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "# Create the classifier\n",
    "ANNClassifier = MLPClassifier(random_state=1, max_iter=500)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "ANNClassifier.fit(X_train_nn, y_train)\n",
    "\n",
    "predictions_test = ANNClassifier.predict(X_test_nn)\n",
    "\n",
    "f1_nn = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_nn = roc_auc_score(y_test, predictions_test)\n",
    "pre_nn = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_nn))\n",
    "print(f\"ROC: {roc_nn}\")\n",
    "print(\"Precision: {}\".format(pre_nn))\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test,predictions_test)\n",
    "\n",
    "nn_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "print(classification_report(y_test,predictions_test))\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/nn_heatmap_79.png\")\n",
    "plt.show()\n",
    "# best_nn_classification_report = nn_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    parameter_grid = {'activation': ['tanh','identity','logistic','relu'],\n",
    "                    'solver': ['adam','lbfgs','sgd'],\n",
    "                    'hidden_layer_sizes': [10,20,30,40,50,60,70,80,90,100]}\n",
    "\n",
    "    cross_validation = StratifiedKFold(n_splits=10, shuffle=True,random_state=1)\n",
    "    \n",
    "    # Create the classifier\n",
    "    nn_classifier = MLPClassifier(random_state=1, max_iter=500)\n",
    "\n",
    "    grid_search = GridSearchCV(nn_classifier,\n",
    "                            param_grid=parameter_grid,\n",
    "                            cv=cross_validation,\n",
    "                            scoring=\"roc_auc\")\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print('Best score: {}'.format(grid_search.best_score_))\n",
    "    print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "    print('Best estimator: {}'.format(grid_search.best_estimator_))\n",
    "    ANNClassifier = grid_search.best_estimator_\n",
    "    yk_pred = ANNClassifier.predict(X_test)\n",
    "\n",
    "    best_nn_classification_report = classification_report(y_test, yk_pred, output_dict=True)\n",
    "\n",
    "    print(\"--- Improved model ---\\n\")\n",
    "    # print(f\"Classification report:\\n{best_nn_classification_report}\\n\")\n",
    "\n",
    "    sb.set(font_scale=1.0)\n",
    "\n",
    "    ax = plt.subplot()\n",
    "\n",
    "    confusion_matrix_ann = confusion_matrix(y_test, yk_pred)\n",
    "    \n",
    "    f1_nn_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_nn_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_nn_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_nn_tun))\n",
    "    print(f\"ROC: {roc_nn_tun}\")\n",
    "    print(f\"Precision: {pre_nn_tun}\")\n",
    "\n",
    "    sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/nn_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Logistic Regression <a class=\"anchor\" id=\"logistic-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "lr.fit(X_scaled, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = lr.predict(X_scaled)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "    \n",
    "scaler_test = preprocessing.StandardScaler().fit(X_test)\n",
    "X_scaled_test = scaler_test.transform(X_test)\n",
    "    \n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = lr.predict(X_scaled_test) \n",
    "lr_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_lr = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_lr = roc_auc_score(y_test, predictions_test)\n",
    "pre_lr = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_lr))\n",
    "print(f\"ROC: {roc_lr}\")\n",
    "print(f\"Precision: {pre_lr}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/lr_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    lr_classifier = LogisticRegression()\n",
    "    \n",
    "    param_grid = {'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "                  'max_iter': [20, 50, 100, 200, 500, 1000],\n",
    "                  'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "                }\n",
    "    \n",
    "    lr_grid_search = GridSearchCV(lr_classifier, scoring=\"roc_auc\", cv=5, param_grid={}, refit = True)\n",
    "    lr_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(lr_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = lr_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_lr = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "    \n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = lr_grid_search.predict(X_test) \n",
    "    lr_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_lr_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_lr_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_lr_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_lr_tun))\n",
    "    print(f\"ROC: {roc_lr_tun}\")\n",
    "    print(f\"Precision: {pre_lr_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_lr = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/lr_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Naive Bayes <a class=\"anchor\" id=\"naive-bayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "# Feature Selection\n",
    "# nb = RFECV(nb, scoring='roc_auc')\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = nb.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = nb.predict(X_test) \n",
    "nb_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_nb = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_nb = roc_auc_score(y_test, predictions_test)\n",
    "pre_nb = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_nb))\n",
    "print(f\"ROC: {roc_nb}\")\n",
    "print(f\"Precision: {pre_nb}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/nb_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    nb_grid_search = GridSearchCV(nb_classifier, scoring=\"roc_auc\", cv=5, param_grid={})\n",
    "    nb_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(nb_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = nb_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_nb = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = nb_grid_search.predict(X_test) \n",
    "    nb_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_nb_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_nb_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_nb_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_nb_tun))\n",
    "    print(f\"ROC: {roc_nb_tun}\")\n",
    "    print(f\"Precision: {pre_nb_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_nb = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/nb_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Random Forest <a class=\"anchor\" id=\"random-forest\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "#from sklearn.feature_selection import SelectFromModel\n",
    "# Feature Selection\n",
    "# rf = RFECV(rf, scoring='f1_macro')\n",
    "#rf = SelectFromModel(RandomForestClassifier(n_estimators = 100))\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "# print(rf.support_)\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = rf.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = rf.predict(X_test)\n",
    "rf_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_rf = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_rf = roc_auc_score(y_test, predictions_test)\n",
    "pre_rf = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_rf))\n",
    "print(f\"ROC: {roc_rf}\")\n",
    "print(f\"Precision: {pre_rf}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/rf_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "    \n",
    "    param_grid = {'bootstrap': [True],\n",
    "                'max_depth': [80, 90, 100, 110],\n",
    "                'max_features': [2, 3],\n",
    "                'min_samples_leaf': [3, 4, 5],\n",
    "                'min_samples_split': [8, 10, 12]\n",
    "                }\n",
    "\n",
    "    rf_grid_search = GridSearchCV(rf_classifier, scoring=\"roc_auc\", cv=5, param_grid=param_grid)\n",
    "    rf_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(rf_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = rf_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_rf = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = rf_grid_search.predict(X_test)\n",
    "    rf_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_rf_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_rf_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_rf_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_rf_tun))\n",
    "    print(f\"ROC: {roc_rf_tun}\")\n",
    "    print(f\"Precision: {pre_rf_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_rf = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/rf_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## XGBoost <a class=\"anchor\" id=\"xgboost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier()\n",
    "\n",
    "# Feature Selection\n",
    "# xgb = RFECV(xgb, scoring='roc_auc')\n",
    "\n",
    "xgb.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = xgb.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_xgb = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_xgb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = xgb.predict(X_test)\n",
    "xgb_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_xgb = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_xgb = roc_auc_score(y_test, predictions_test)\n",
    "pre_xgb = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_xgb))\n",
    "print(f\"ROC: {roc_xgb}\")\n",
    "print(f\"Precision: {pre_xgb}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_xgb = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_xgb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/xgb_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-8\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    xgb_classifier = XGBClassifier(metrics='rmse')\n",
    "    \n",
    "    param_grid = {'max-depth': [3, 6, 10],\n",
    "                  'n_estimators': [100, 300, 500, 1000],\n",
    "                  'learning_rate': [0.01, 0.05, 0.1],\n",
    "                  'colsample_bytree': [0.3, 0.7]\n",
    "                }\n",
    "\n",
    "    xgb_grid_search = GridSearchCV(xgb_classifier, scoring='roc_auc', cv=5, param_grid=param_grid)\n",
    "\n",
    "#     params = { 'max_depth': [3, 6, 10, 20],\n",
    "#            'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
    "#            'subsample': [0.5, 0.7, 1],\n",
    "#            'colsample_bytree': [0.4, 0.7, 1],\n",
    "#            'colsample_bylevel': [0.4, 0.7, 1],\n",
    "#            'n_estimators': [100, 500, 1000]}\n",
    "#     xgb_classifier = XGBRegressor(seed = 20)\n",
    "#     xgb_grid_search = RandomizedSearchCV(estimator=xgb_classifier,\n",
    "#                              param_distributions=params,\n",
    "#                              scoring='roc_auc',\n",
    "#                              n_iter=25)\n",
    "\n",
    "    xgb_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(xgb_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = xgb_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_xgb = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_xgb, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = xgb_grid_search.predict(X_test)\n",
    "    xgb_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_xgb_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_xgb_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_xgb_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_xgb_tun))\n",
    "    print(f\"ROC: {roc_xgb_tun}\")\n",
    "    print(f\"Precision: {pre_xgb_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_xgb = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_xgb, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/xgb_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Ada Boost <a class=\"anchor\" id=\"ada-boost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = AdaBoostClassifier()\n",
    "\n",
    "# Feature Selection\n",
    "# boost = RFECV(boost, scoring='roc_auc')\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = boost.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_boost = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_boost, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = boost.predict(X_test) \n",
    "boost_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "f1_boost = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "roc_boost = roc_auc_score(y_test, predictions_test)\n",
    "pre_boost = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "print(\"F1 Score: {}\".format(f1_boost))\n",
    "print(f\"ROC: {roc_boost}\")\n",
    "print(f\"Precision: {pre_boost}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_boost = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_boost, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.savefig(\"graphics/ada_heatmap_79.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    #ada_classifier = AdaBoostClassifier(random_state=0)\n",
    "    \n",
    "    param_grid = {'base_estimator__max_depth':[i for i in range(2,11,2)],\n",
    "              'base_estimator__min_samples_leaf':[5,10],\n",
    "              'n_estimators':[10,50,250,1000],\n",
    "              'learning_rate':[0.01,0.1]}\n",
    "\n",
    "\n",
    "    DTC = DecisionTreeClassifier(random_state = 11, max_features = \"auto\", class_weight = \"balanced\",max_depth = None)\n",
    "\n",
    "    ada_classifier = AdaBoostClassifier(base_estimator = DTC)\n",
    "\n",
    "    # run grid search\n",
    "    ada_grid_search = GridSearchCV(ada_classifier, param_grid=param_grid, scoring = 'roc_auc')\n",
    "    \n",
    "    #ada_grid_search = GridSearchCV(ada_classifier, scoring=\"roc_auc\", cv=5, param_grid=grid)\n",
    "    ada_grid_search.fit(X_train, y_train)\n",
    "    print('Best score: {}'.format(ada_grid_search.best_score_))\n",
    "\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TRAIN\")\n",
    "    predictions_train = ada_grid_search.predict(X_train)\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "    print(\"Precision: {}\".format(precision_score(y_train, predictions_train, pos_label=-1)))\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_train, predictions_train, output_dict=True))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_ada = confusion_matrix(y_train, predictions_train)\n",
    "    sb.heatmap(confusion_matrix_ada, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.show()\n",
    "\n",
    "    print(53 * '=')\n",
    "    print(\"TEST\")\n",
    "    predictions_test = ada_grid_search.predict(X_test) \n",
    "    ada_tun_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "    f1_boost_tun = f1_score(y_test, predictions_test, pos_label=-1)\n",
    "    roc_boost_tun = roc_auc_score(y_test, predictions_test)\n",
    "    pre_boost_tun = precision_score(y_test, predictions_test, pos_label=-1)\n",
    "    print(\"F1 Score: {}\".format(f1_boost_tun))\n",
    "    print(f\"ROC: {roc_boost_tun}\")\n",
    "    print(f\"Precision: {pre_boost_tun}\")\n",
    "    print(\"Classification Report: \")\n",
    "    print(classification_report(y_test, predictions_test))\n",
    "    sb.set(font_scale=1.0)\n",
    "    ax = plt.subplot()\n",
    "    confusion_matrix_ada = confusion_matrix(y_test, predictions_test)\n",
    "    sb.heatmap(confusion_matrix_ada, annot=True, ax=ax, fmt=\"g\")\n",
    "    ax.set_xlabel('Predicted Grades');\n",
    "    ax.set_ylabel('Observed Grades');\n",
    "    ax.set_title('Confusion Matrix');\n",
    "    plt.savefig(\"graphics/ada_tuning_heatmap_79.png\")    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "## Results Analysis <a id=\"results-analysis\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = go.Figure(data=[go.Table(header=dict(values=['Algorithm', 'Accuracy Normal', 'F1 Score Normal', 'ROC AUC Score Normal', 'Precision']),\n",
    "                 cells=dict(values=[['DT', 'KNN', 'SVM', 'NN', 'LR', 'NB', 'RF', 'XGB', 'ADA BOOST'], \n",
    "                                    [dtc_classification_report['accuracy'], knn_classification_report['accuracy'], svm_classification_report['accuracy'], nn_classification_report['accuracy'], lr_classification_report['accuracy'], nb_classification_report['accuracy'], rf_classification_report['accuracy'], xgb_classification_report['accuracy'], boost_classification_report['accuracy']], \n",
    "                                    [f1_dt, f1_knn, f1_svm, f1_nn, f1_lr, f1_nb, f1_rf, f1_xgb, f1_boost], \n",
    "                                    [roc_dt, roc_knn, roc_svm, roc_nn, roc_lr, roc_nb, roc_rf, roc_xgb, roc_boost],\n",
    "                                    [pre_dt, pre_knn, pre_svm, pre_nn, pre_lr, pre_nb, pre_rf, pre_xgb, pre_boost]\n",
    "                                   ]))\n",
    "                     ])\n",
    "\n",
    "table.write_image(\"graphics/table_79.png\")\n",
    "table.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_param_tunning:\n",
    "    table1 = go.Figure(data=[go.Table(header=dict(values=['Algorithm', 'Accuracy Normal', 'F1 Score Normal', 'ROC AUC Score Normal', 'Precision']),\n",
    "                 cells=dict(values=[['DT', 'KNN', 'SVM', 'NN', 'LR', 'NB', 'RF', 'XGB', 'ADA BOOST'], \n",
    "                                    [dtc_tun_classification_report['accuracy'], knn_tun_classification_report['accuracy'], svm_tun_classification_report['accuracy'], best_nn_classification_report['accuracy'], lr_tun_classification_report['accuracy'], nb_tun_classification_report['accuracy'], rf_tun_classification_report['accuracy'], xgb_tun_classification_report['accuracy'], ada_tun_classification_report['accuracy']], \n",
    "                                    [f1_dt_tun, f1_knn_tun, f1_svm_tun, f1_nn_tun, f1_lr_tun, f1_nb_tun, f1_rf_tun, f1_xgb_tun, f1_boost_tun], \n",
    "                                    [roc_dt_tun, roc_knn_tun, roc_svm_tun, roc_nn_tun, roc_lr_tun, roc_nb_tun, roc_rf_tun, roc_xgb_tun, roc_boost_tun],\n",
    "                                    [pre_dt_tun, pre_knn_tun, pre_svm_tun, pre_nn_tun, pre_lr_tun, pre_nb_tun, pre_rf_tun, pre_xgb_tun, pre_boost_tun]\n",
    "                                   ]))\n",
    "                                 ])\n",
    "\n",
    "    table1.write_image(\"graphics/table_tuning_79.png\")        \n",
    "    table1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives LR\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [62,62,62,62,61,58,61,59,55,60,59,66,66,66,63,62,58,59,59]\n",
    "plt.plot(x,y)\n",
    "plt.title('True Positives Logistic Regression')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('True Positives')\n",
    "plt.savefig(\"graphics/true_positives_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives RF\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [62,61,60,62,58,62,60,61,61,61,62,62,62,65,64,62,66,65,67]\n",
    "plt.plot(x,y)\n",
    "plt.title('True Positives Random Forest')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('True Positives')\n",
    "plt.savefig(\"graphics/true_positives_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives XGB\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [63,62,62,63,66,63,65,64,65,65,64,66,64,65,66,64,63,67,67]\n",
    "plt.plot(x,y)\n",
    "plt.title('True Positives XGBoost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('True Positives')\n",
    "plt.savefig(\"graphics/true_positives_xgb.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives ADA\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [65,66,66,60,60,64,65,67,67,66,67,61,65,67,66,67,67,69,69]\n",
    "plt.plot(x,y)\n",
    "plt.title('True Positives Ada Boost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('True Positives')\n",
    "plt.savefig(\"graphics/true_positives_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positives all\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [62,62,62,62,61,58,61,59,55,60,59,66,66,66,63,62,58,59,59]\n",
    "rf = [62,61,60,62,58,62,60,61,61,61,62,62,62,65,64,62,66,65,67]\n",
    "xgb = [63,62,62,63,66,63,65,64,65,65,64,66,64,65,66,64,63,67,67]\n",
    "ada = [65,66,66,60,60,64,65,67,67,66,67,61,65,67,66,67,67,69,69]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('True Positives Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('True Positives')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/true_positives_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives LR\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [9,9,9,9,10,13,10,12,16,11,12,5,5,5,8,9,13,12,12]\n",
    "plt.plot(x,y)\n",
    "plt.title('False Positives Logistic Regression')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('False Positives')\n",
    "plt.savefig(\"graphics/false_positives_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives RF\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [9,10,11,9,13,9,11,10,10,10,9,9,9,6,7,9,5,6,4]\n",
    "plt.plot(x,y)\n",
    "plt.title('False Positives Random Forest')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('False Positives')\n",
    "plt.savefig(\"graphics/false_positives_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives XGB\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [8,9,8,8,5,8,6,7,6,6,7,5,7,6,5,7,8,4,4]\n",
    "plt.plot(x,y)\n",
    "plt.title('False Positives XGBoost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('False Positives')\n",
    "plt.savefig(\"graphics/false_positives_xgb.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives ADA\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "y = [6,5,5,11,11,7,6,4,4,5,4,10,6,4,5,4,4,2,2]\n",
    "plt.plot(x,y)\n",
    "plt.title('False Positives Ada Boost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('False Positives')\n",
    "plt.savefig(\"graphics/false_positives_ada.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# False Positives all\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [9,9,9,9,10,13,10,12,16,11,12,5,5,5,8,9,13,12,12]\n",
    "rf = [9,10,11,9,13,9,11,10,10,10,9,9,9,6,7,9,5,6,4]\n",
    "xgb = [8,9,8,8,5,8,6,7,6,6,7,5,7,6,5,7,8,4,4]\n",
    "ada = [6,5,5,11,11,7,6,4,4,5,4,10,6,4,5,4,4,2,2]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('False Positives Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('False Positives')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/false_positives_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy All\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [0.8780,0.8780,0.8780,0.8780,0.8659,0.8170,0.8659,0.8415,0.7683,0.8415,0.8171,0.9024,0.9024,0.8902,0.8659,0.8659,0.8049,0.8171,0.8171]\n",
    "rf = [0.8537,0.8293,0.8049,0.8293,0.7805,0.8293,0.7927,0.8049,0.8049,0.8049,0.8171,0.8171,0.8171,0.8537,0.8537,0.8049,0.8659,0.8537,0.8780]\n",
    "xgb = [0.8415,0.8415,0.8780,0.8659,0.9024,0.8537,0.8902,0.8780,0.8902,0.8780,0.8293,0.8780,0.8537,0.8659,0.8537,0.8537,0.8537,0.8902,0.8902]\n",
    "ada = [0.8537,0.8659,0.8780,0.8049,0.8049,0.8415,0.8537,0.8780,0.8659,0.8659,0.8902,0.7927,0.8415,0.8780,0.8659,0.8415,0.8659,0.8902,0.9024]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/accuracy_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score All\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [0.9254,0.9254,0.9254,0.9254,0.9173,0.8855,0.9173,0.9008,0.8527,0.9023,0.8872,0.9429,0.9429,0.9362,0.9197,0.9185,0.8788,0.8872,0.8872]\n",
    "rf = [0.9118,0.8971,0.8824,0.8986,0.8657,0.8986,0.8759,0.8841,0.8841,0.8841,0.8921,0.8921,0.8921,0.9155,0.9143,0.8857,0.9231,0.9155,0.9306]\n",
    "xgb = [0.9065,0.9051,0.9265,0.9197,0.9429,0.9130,0.9353,0.9275,0.9353,0.9286,0.9014,0.9296,0.9143,0.9220,0.9167,0.9143,0.9130,0.9371,0.9371]\n",
    "ada = [0.9155,0.9231,0.9296,0.8824,0.8824,0.9078,0.9155,0.9306,0.9241,0.9231,0.9371,0.8777,0.9091,0.9306,0.9231,0.9116,0.9241,0.9388,0.9452]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/f1_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC AUC Score All\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [0.8912,0.8912,0.8912,0.8912,0.8841,0.7433,0.8841,0.8700,0.7510,0.8316,0.7791,0.8284,0.8284,0.7830,0.8073,0.8457,0.7721,0.7791,0.7791]\n",
    "rf = [0.8003,0.7478,0.6953,0.7093,0.6812,0.7093,0.6498,0.6569,0.6569,0.6569,0.6639,0.6639,0.6639,0.6850,0.7234,0.6184,0.6921,0.6850,0.6991]\n",
    "xgb = [0.7164,0.7548,0.8528,0.8073,0.8284,0.7618,0.8214,0.8143,0.8214,0.7759,0.6325,0.7375,0.7234,0.7305,0.6466,0.7234,0.7618,0.7446,0.7446]\n",
    "ada = [0.6850,0.6921,0.7375,0.6953,0.6923,0.6780,0.6850,0.6991,0.6536,0.6921,0.7446,0.6114,0.6396,0.6991,0.6921,0.5627,0.6536,0.6677,0.7132]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('ROC AUC Score Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/roc_auc_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision All\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "lr = [0.9841,0.9841,0.9841,0.9841,0.9839,0.9667,0.9839,0.9833,0.9483,0.9677,0.9516,0.9565,0.9565,0.9429,0.9545,0.9688,0.9508,0.9516,0.9516]\n",
    "rf = [0.9538,0.9385,0.9231,0.9254,0.9206,0.9254,0.9091,0.9104,0.9104,0.9104,0.9118,0.9118,0.9118,0.9155,0.9275,0.8986,0.9167,0.9155,0.9178]\n",
    "xgb = [0.9265,0.9394,0.9692,0.9546,0.9565,0.9403,0.9559,0.9552,0.9559,0.9420,0.9014,0.9296,0.9275,0.9286,0.9041,0.9275,0.9403,0.9306,0.9306]\n",
    "ada = [0.9155,0.9167,0.9296,0.9231,0.9231,0.9143,0.9155,0.9178,0.9054,0.9167,0.9306,0.8971,0.9028,0.9178,0.9167,0.8816,0.9054,0.9079,0.92]\n",
    "plt.plot(x,lr,label=\"Logistic Regression\")\n",
    "plt.plot(x,rf,label=\"Random Forest\")\n",
    "plt.plot(x,xgb,label=\"XGBoost\")\n",
    "plt.plot(x,ada,label=\"Ada Boost\")\n",
    "plt.title('Precision Comparison')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/precision_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison all LR\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "acc = [0.8780,0.8780,0.8780,0.8780,0.8659,0.8170,0.8659,0.8415,0.7683,0.8415,0.8171,0.9024,0.9024,0.8902,0.8659,0.8659,0.8049,0.8171,0.8171]\n",
    "f1 = [0.9254,0.9254,0.9254,0.9254,0.9173,0.8855,0.9173,0.9008,0.8527,0.9023,0.8872,0.9429,0.9429,0.9362,0.9197,0.9185,0.8788,0.8872,0.8872]\n",
    "roc = [0.8912,0.8912,0.8912,0.8912,0.8841,0.7433,0.8841,0.8700,0.7510,0.8316,0.7791,0.8284,0.8284,0.7830,0.8073,0.8457,0.7721,0.7791,0.7791]\n",
    "pre = [0.9841,0.9841,0.9841,0.9841,0.9839,0.9667,0.9839,0.9833,0.9483,0.9677,0.9516,0.9565,0.9565,0.9429,0.9545,0.9688,0.9508,0.9516,0.9516]\n",
    "plt.plot(x,acc,label=\"Accuracy\")\n",
    "plt.plot(x,f1,label=\"F1\")\n",
    "plt.plot(x,roc,label=\"ROC AUC\")\n",
    "plt.plot(x,pre,label=\"Precision\")\n",
    "plt.title('Parameters Comparison Logistic Regression')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/comparison_all_lr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison all RF\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "acc = [0.8537,0.8293,0.8049,0.8293,0.7805,0.8293,0.7927,0.8049,0.8049,0.8049,0.8171,0.8171,0.8171,0.8537,0.8537,0.8049,0.8659,0.8537,0.8780]\n",
    "f1 = [0.9118,0.8971,0.8824,0.8986,0.8657,0.8986,0.8759,0.8841,0.8841,0.8841,0.8921,0.8921,0.8921,0.9155,0.9143,0.8857,0.9231,0.9155,0.9306]\n",
    "roc = [0.8003,0.7478,0.6953,0.7093,0.6812,0.7093,0.6498,0.6569,0.6569,0.6569,0.6639,0.6639,0.6639,0.6850,0.7234,0.6184,0.6921,0.6850,0.6991]\n",
    "pre = [0.9538,0.9385,0.9231,0.9254,0.9206,0.9254,0.9091,0.9104,0.9104,0.9104,0.9118,0.9118,0.9118,0.9155,0.9275,0.8986,0.9167,0.9155,0.9178]\n",
    "plt.plot(x,acc,label=\"Accuracy\")\n",
    "plt.plot(x,f1,label=\"F1\")\n",
    "plt.plot(x,roc,label=\"ROC AUC\")\n",
    "plt.plot(x,pre,label=\"Precision\")\n",
    "plt.title('Parameters Comparison Random Forest')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/comparison_all_rf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison all XGB\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "acc = [0.8415,0.8415,0.8780,0.8659,0.9024,0.8537,0.8902,0.8780,0.8902,0.8780,0.8293,0.8780,0.8537,0.8659,0.8537,0.8537,0.8537,0.8902,0.8902]\n",
    "f1 = [0.9065,0.9051,0.9265,0.9197,0.9429,0.9130,0.9353,0.9275,0.9353,0.9286,0.9014,0.9296,0.9143,0.9220,0.9167,0.9143,0.9130,0.9371,0.9371]\n",
    "roc = [0.7164,0.7548,0.8528,0.8073,0.8284,0.7618,0.8214,0.8143,0.8214,0.7759,0.6325,0.7375,0.7234,0.7305,0.6466,0.7234,0.7618,0.7446,0.7446]\n",
    "pre = [0.9265,0.9394,0.9692,0.9546,0.9565,0.9403,0.9559,0.9552,0.9559,0.9420,0.9014,0.9296,0.9275,0.9286,0.9041,0.9275,0.9403,0.9306,0.9306]\n",
    "plt.plot(x,acc,label=\"Accuracy\")\n",
    "plt.plot(x,f1,label=\"F1\")\n",
    "plt.plot(x,roc,label=\"ROC AUC\")\n",
    "plt.plot(x,pre,label=\"Precision\")\n",
    "plt.title('Parameters Comparison XGBoost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/comparison_all_xgb.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison all ADA\n",
    "x = [16,17,18,19,20,21,22,25,30,35,40,45,50,55,60,65,70,75,79]\n",
    "acc = [0.8537,0.8659,0.8780,0.8049,0.8049,0.8415,0.8537,0.8780,0.8659,0.8659,0.8902,0.7927,0.8415,0.8780,0.8659,0.8415,0.8659,0.8902,0.9024]\n",
    "f1 = [0.9155,0.9231,0.9296,0.8824,0.8824,0.9078,0.9155,0.9306,0.9241,0.9231,0.9371,0.8777,0.9091,0.9306,0.9231,0.9116,0.9241,0.9388,0.9452]\n",
    "roc = [0.6850,0.6921,0.7375,0.6953,0.6923,0.6780,0.6850,0.6991,0.6536,0.6921,0.7446,0.6114,0.6396,0.6991,0.6921,0.5627,0.6536,0.6677,0.7132]\n",
    "pre = [0.9155,0.9167,0.9296,0.9231,0.9231,0.9143,0.9155,0.9178,0.9054,0.9167,0.9306,0.8971,0.9028,0.9178,0.9167,0.8816,0.9054,0.9079,0.92]\n",
    "plt.plot(x,acc,label=\"Accuracy\")\n",
    "plt.plot(x,f1,label=\"F1\")\n",
    "plt.plot(x,roc,label=\"ROC AUC\")\n",
    "plt.plot(x,pre,label=\"Precision\")\n",
    "plt.title('Parameters Comparison ADA Boost')\n",
    "plt.xlabel('Features Number')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.savefig(\"graphics/comparison_all_ada.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Apply Model <a class=\"anchor\" id=\"apply-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_data_no_ids.drop(columns=['status', 'loan_id'])\n",
    "all_ids_comp = test_data['loan_id'].values\n",
    "\n",
    "pred_comp = xgb_grid_search.predict_proba(test_inputs)\n",
    "\n",
    "pred_comp = pd.DataFrame(pred_comp, columns=['col2', 'Predicted'])\n",
    "\n",
    "pred_comp.drop('col2', axis=1, inplace=True)\n",
    "all_ids_comp = pd.DataFrame(all_ids_comp, columns=['Id'])\n",
    "# pred_comp['Predicted'] = [(1 - pred_comp['Predicted'][n]) for n in range(len(pred_comp))]\n",
    "results = pd.concat([all_ids_comp, pred_comp], axis=1)\n",
    "results = results.rename(columns={\"loan_id\":\"Id\"})\n",
    "# results.loc[results['Predicted']<0.03,'Predicted']=0\n",
    "# results.loc[results['Predicted']>0.97,'Predicted']=1\n",
    "results.to_csv('results.csv', index = False)\n",
    "\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
