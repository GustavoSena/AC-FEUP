{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9803afc",
   "metadata": {},
   "source": [
    "**TO-DO**\n",
    "* verificar se para todos os valores nulos na operation, todos os k_symbol são interested credited\n",
    "* verificar se a k_symbol acrescenta informação relevante à operation (sem ser os valores nulos corresponderem ao interested credited), visto que esta apenas especifica mais algumas operações\n",
    "* agrupar withdrawal in cash e withdrawal do type (testar juntos e separados para ver os resultados)\n",
    "* retirar coluna account - tem muitos valores a zero (verificar, pois poderá ser devido a um erro) (testar primeiro sem valores a zero e depois sem a coluna)\n",
    "* retirar coluna bank\n",
    "* provavelmente retirar a coluna type, pois poderá não acrescentar valor aos dados nada de relevante que já não esteja na coluna operation (testar). Coluna type é uma generalização da operation logo apenas simplifica a informação e não acrescenta nada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c063735",
   "metadata": {},
   "source": [
    "## Table Of Contents <a id=\"index\"></a>\n",
    "\n",
    "\n",
    "- [Dataset managing](#dataset)  \n",
    "- [District Data](#district-data)\n",
    "- [Matrix](#matrix)\n",
    "  \n",
    "\n",
    "#### Models\n",
    "- [**Decision Tree**](#decision-tree)\n",
    "    - [**Parameter Tunning**](#parameter-tunning)\n",
    "- [**K-Nearest Neighbor**](#k-nearest-neighbor)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-2)  \n",
    "- [**Support-Vector Machines**](#support-vector-machines)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-3)\n",
    "- [**Neural Networks**](#neural-networks)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f44dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e266b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(x, missing = \"\"):\n",
    "    return pd.read_csv('Dataset/' + x + '.csv', sep = ';', low_memory = False, na_values = missing_values).rename(str.strip, axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d3892",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = ['?', 'NA', '']\n",
    "account_data = dataset('account', missing_values)\n",
    "client_data = dataset('client', missing_values)\n",
    "disp_data = dataset('disp', missing_values)\n",
    "district_data = dataset('district', missing_values)\n",
    "card_train = dataset('card_train', missing_values)\n",
    "card_test = dataset('card_test')\n",
    "loan_train = dataset('loan_train', missing_values)\n",
    "loan_test = dataset('loan_test')\n",
    "trans_train = dataset('trans_train', missing_values)\n",
    "trans_test = dataset('trans_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545f249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing name and region from district\n",
    "district_data.drop(['name', 'region'], axis=1, inplace=True)\n",
    "\n",
    "# only owner can issue permanent orders and ask for a loan\n",
    "disp_owners = disp_data[disp_data.type.eq('OWNER')]\n",
    "disp_owners.drop(['type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783c540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize birthday dates and add a new column (Genre) to distinguish\n",
    "\n",
    "birth_dates = client_data['birth_number']\n",
    "dates_parsed = []\n",
    "genre = []\n",
    "for date in birth_dates:\n",
    "    month = int(str(date)[2:4])\n",
    "    if month > 12:\n",
    "        genre.append(0)\n",
    "        month = month - 50\n",
    "        if month < 10:\n",
    "            month = '0' + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "    else:\n",
    "        #print('AAAA: ' + str(month))\n",
    "        if month < 10:\n",
    "            month = '0' + str(month)\n",
    "            #print('BBBB: ' + str(month))\n",
    "        else:\n",
    "            month = str(month)\n",
    "        genre.append(1)\n",
    "    dates_parsed.append(str(date)[:2] + '-' + month + '-' + str(date)[4:])\n",
    "    \n",
    "ages = []\n",
    "for date in dates_parsed:\n",
    "    born_year = '19' + date[:2]\n",
    "    age = 2021 - int(born_year)\n",
    "    ages.append(age)\n",
    "    \n",
    "client_data = client_data.drop(['birth_number'], axis = 1)\n",
    "client_data['age'] = ages\n",
    "client_data['genre'] = genre\n",
    "client_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221ce199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train dataset\n",
    "\n",
    "train_data = loan_train\n",
    "train_data = pd.merge(train_data, trans_train, on = 'account_id', suffixes = ('', '_trans'))\n",
    "train_data = pd.merge(train_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "#train_data = train_data.dropna()\n",
    "train_data = pd.merge(train_data, district_data.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "train_data = pd.merge(train_data, disp_owners, on = 'account_id', suffixes = ('', '_disp'))\n",
    "train_data = pd.merge(train_data, card_train, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "train_data = pd.merge(train_data, client_data, on = 'client_id', suffixes = ('', '_client'))\n",
    "train_data = train_data.drop(['district_id_client'], axis=1)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654da02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d9ffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test dataset\n",
    "\n",
    "test_data = loan_test\n",
    "test_data = pd.merge(test_data, trans_test, on = 'account_id', suffixes = ('', '_trans'))\n",
    "test_data = pd.merge(test_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "test_data = pd.merge(test_data, district_data.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "test_data = pd.merge(test_data, disp_owners, on = 'account_id', suffixes = ('', '_disp'))\n",
    "test_data = pd.merge(test_data, card_test, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "test_data = pd.merge(test_data, client_data, on = 'client_id', suffixes = ('', '_client'))\n",
    "test_data = test_data.drop(['district_id_client'], axis=1)\n",
    "test_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1549a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all dates from data, because only the number doesn't make sense. Probably transform it to age?\n",
    "train_data.drop(['date', 'date_trans', 'date_account'], axis=1, inplace=True)\n",
    "test_data.drop(['date', 'date_trans', 'date_account'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bc1d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values for each attribute\n",
    "train_data.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "plt.ylabel('Null values');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null values unemploymant rate in \\'95:' + str(district_data['unemploymant rate \\'95'].isnull().sum()))\n",
    "print()\n",
    "print('Null values no. of commited crimes \\'95 :' + str(district_data['no. of commited crimes \\'95'].isnull().sum()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48afdc5",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### District Data <a id=\"district-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c77f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "district_scatter_plot = sb.PairGrid(district_data)\n",
    "district_scatter_plot.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868332d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,6))\n",
    "plt.title('Distribution of district\\'s unemploymant rate in \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.hist(district_data['unemploymant rate \\'95'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a35781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40,6))\n",
    "plt.title('Distribution of district\\'s no. of commited crimes \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.hist(district_data['no. of commited crimes \\'95'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66a52bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in district's unemploymant rate in '95 and district's no. of commited crimes '95\n",
    "# See if it is better to use median or mean \n",
    "\n",
    "train_data['unemploymant rate \\'95'].fillna(train_data['unemploymant rate \\'95'].median(), inplace=True)\n",
    "\n",
    "train_data['no. of commited crimes \\'95'].fillna(train_data['no. of commited crimes \\'95'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2776d449",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.operation.value_counts())\n",
    "print('Null values: ' + str(train_data.operation.isnull().sum()))\n",
    "print()\n",
    "print(train_data.k_symbol.value_counts())\n",
    "print('Null values: ' + str(train_data.k_symbol.isnull().sum()))\n",
    "print()\n",
    "print(train_data.bank.value_counts())\n",
    "print('Null values: ' + str(train_data.bank.isnull().sum()))\n",
    "print()\n",
    "print(train_data.type_card.value_counts())\n",
    "print('Null values: ' + str(train_data.type_card.isnull().sum()))\n",
    "print()\n",
    "print(train_data.type.value_counts())\n",
    "print('Null values: ' + str(train_data.type.isnull().sum()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d413b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_null = train_data[train_data['operation'].isnull()]\n",
    "operations_null.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b660b9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_null.k_symbol.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02dda18",
   "metadata": {},
   "source": [
    "All rows that have 'operation' column with null value have 'interested credited' in 'k_symbol' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87326dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9973447e",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### Dataset Managing <a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b6178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be checked\n",
    "option = 1\n",
    "if option ==1:\n",
    "    train_data.drop(['bank', 'k_symbol', 'account', 'operation', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['bank', 'k_symbol', 'account', 'operation', 'issued'], axis=1, inplace=True)\n",
    "#replaced withdrawal in cash with only withdrawal\n",
    "elif option==2:\n",
    "    train_data.drop(['bank', 'k_symbol', 'account', 'operation', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['bank', 'k_symbol', 'account', 'operation', 'issued'], axis=1, inplace=True)\n",
    "    train_data.replace('withdrawal in cash', 'withdrawal', inplace=True)\n",
    "    test_data.replace('withdrawal in cash', 'withdrawal', inplace=True)\n",
    "#filled nulls from operation with k_symbol\n",
    "elif option==3:\n",
    "    train_data['operation'].fillna(train_data['k_symbol'], inplace=True)\n",
    "    test_data['operation'].fillna(test_data['k_symbol'], inplace=True)\n",
    "    train_data.drop(['bank', 'k_symbol', 'account', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['bank', 'k_symbol', 'account', 'issued'], axis=1, inplace=True)\n",
    "#joined operation and k_symbol\n",
    "elif option==4:\n",
    "    train_data['operation'].fillna(train_data['k_symbol'], inplace=True)\n",
    "    test_data['operation'].fillna(test_data['k_symbol'], inplace=True)\n",
    "    train_data['k_symbol'].fillna(train_data['operation'], inplace=True)\n",
    "    test_data['k_symbol'].fillna(test_data['operation'], inplace=True)\n",
    "    train_data['operation']=train_data['operation'] + ' ' + train_data['k_symbol']\n",
    "    test_data['operation']=test_data['operation'] + ' ' + test_data['k_symbol']\n",
    "    train_data[\"operation\"] = train_data[\"operation\"].apply(lambda x: ' '.join(pd.unique(x.split())))\n",
    "    test_data[\"operation\"] = test_data[\"operation\"].apply(lambda x: ' '.join(pd.unique(x.split())))\n",
    "    train_data.drop(['bank', 'k_symbol', 'account', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['bank', 'k_symbol', 'account', 'issued'], axis=1, inplace=True)\n",
    "#joined operation and k_symbol and dropped type\n",
    "elif option==5:\n",
    "    train_data['operation'].fillna(train_data['k_symbol'], inplace=True)\n",
    "    test_data['operation'].fillna(test_data['k_symbol'], inplace=True)\n",
    "    train_data['k_symbol'].fillna(train_data['operation'], inplace=True)\n",
    "    test_data['k_symbol'].fillna(test_data['operation'], inplace=True)\n",
    "    train_data['operation']=train_data['operation'] + ' ' + train_data['k_symbol']\n",
    "    test_data['operation']=test_data['operation'] + ' ' + test_data['k_symbol']\n",
    "    train_data[\"operation\"] = train_data[\"operation\"].apply(lambda x: ' '.join(pd.unique(x.split())))\n",
    "    test_data[\"operation\"] = test_data[\"operation\"].apply(lambda x: ' '.join(pd.unique(x.split())))\n",
    "    train_data.drop(['bank', 'k_symbol', 'account', 'issued', 'type'], axis=1, inplace=True)\n",
    "    test_data.drop(['bank', 'k_symbol', 'account', 'issued', 'type'], axis=1, inplace=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce354c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=['frequency'], dtype=bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['frequency'], dtype=bool)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['type_card'], dtype = bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['type_card'], dtype = bool)\n",
    "\n",
    "# train_data = pd.get_dummies(train_data, columns=['type_disp'], dtype = bool)\n",
    "# test_data = pd.get_dummies(test_data, columns=['type_disp'], dtype = bool)\n",
    "\n",
    "if option != 5:\n",
    "    train_data = pd.get_dummies(train_data, columns=['type'], dtype = bool)\n",
    "    test_data = pd.get_dummies(test_data, columns=['type'], dtype = bool)\n",
    "\n",
    "if option == 1 or option == 2:\n",
    "    train_data = pd.get_dummies(train_data, columns=['operation'], dtype = bool)\n",
    "    test_data = pd.get_dummies(test_data, columns=['operation'], dtype = bool)\n",
    "\n",
    "test_data = test_data.drop_duplicates(subset=['loan_id'], keep='first')\n",
    "\n",
    "#train_data = pd.get_dummies(train_data)\n",
    "#test_data = pd.get_dummies(test_data)\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe1fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbf290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_inputs = test_data.drop(columns=[\"loan_id\", \"status\"])\n",
    "test_data = test_data.drop(columns=[\"status\"])\n",
    "all_ids_comp = test_data['loan_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35333bad",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### Matrix <a id=\"matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b459252",
   "metadata": {},
   "source": [
    "train_data_w /test_data_w : train/test data where withdrawals in cash are replaced by withdrawals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ids\n",
    "#train_data_no_ids = train_data.drop(['loan_id', 'account_id', 'district_id', 'disp_id', 'client_id', 'card_id', 'trans_id'], axis=1)\n",
    "train_data_no_ids = train_data.drop(['client_id', 'account_id', 'district_id', 'disp_id', 'card_id', 'trans_id'], axis=1)\n",
    "\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = train_data_no_ids.corr().abs()\n",
    "plt.figure(figsize = (20,6))\n",
    "sb.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "train_data_no_ids.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(\"{} Dropped columns: {}\".format(len(to_drop), to_drop) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27413f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with nan values for status\n",
    "#train_data_no_ids.dropna(subset=[\"status\"], inplace=True)\n",
    "\n",
    "#print(no_ids.drop_duplicates(inplace=True)\n",
    "print(train_data_no_ids[\"status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882eb5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb53b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1da43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing labels and creating another dataset for them\n",
    "train_data_no_ids = train_data_no_ids.dropna()\n",
    "default_ind_no = train_data_no_ids.loc[train_data_no_ids['status'] == -1]\n",
    "default_ind_yes = train_data_no_ids.loc[train_data_no_ids['status'] == 1]\n",
    "\n",
    "df_minority_upsampled = resample(default_ind_yes, replace=True, n_samples=len(default_ind_no),random_state=123)\n",
    "\n",
    "train_data_no_ids = pd.concat([default_ind_no, df_minority_upsampled])\n",
    "\n",
    "train_data_no_ids.status.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d239de",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_data_no_ids[train_data_no_ids.columns.drop(['loan_id'])]\n",
    "all_labels = train_data_no_ids['status'].values\n",
    "\n",
    "competition_inputs = test_data.drop(columns=[\"loan_id\"])\n",
    "all_ids_comp = test_data['loan_id'].values\n",
    "\n",
    "#all_inputs = train_data_no_ids.iloc[:, :-1].values\n",
    "#all_labels = train_data_no_ids.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe57de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs.status.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3002ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test dataset with 25% of the credit_data_subset\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)\n",
    "#(X_train, X_test, y_train, y_test) = train_test_split(all_inputs, all_labels, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc91494",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc37464b",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Decision Tree <a class=\"anchor\" id=\"decision-tree\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "dtc_prediction = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "dtc_classification_report = classification_report(y_test, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Classification report:\\n{classification_report(y_test, dtc_prediction)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    " \n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_dtc = confusion_matrix(y_test, dtc_prediction)\n",
    "\n",
    "sb.heatmap(confusion_matrix_dtc, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747e18d6",
   "metadata": {},
   "source": [
    "\n",
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5399d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "                  'splitter': ['best', 'random'],\n",
    "                  'max_depth': range(10, 20),\n",
    "                  'max_features': range(10,20)}\n",
    "\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10,\n",
    "                           verbose=4,\n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9cb844",
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_classifier = grid_search.best_estimator_\n",
    "dtc_prediction = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "best_dtc_classification_report = classification_report(y_test, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(\"--- Improved model ---\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, dtc_prediction)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_dtc = confusion_matrix(y_test, dtc_prediction)\n",
    "\n",
    "sb.heatmap(confusion_matrix_dtc, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abddc7c",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## K-Nearest Neighbor <a class=\"anchor\" id=\"k-nearest-neighbor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03551cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn_prediction = knn.predict(X_test)\n",
    "\n",
    "knn_classification_report = classification_report(y_test, knn_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Classification report:\\n{classification_report(y_test, knn_prediction, labels=np.unique(y_train))}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_knn = confusion_matrix(y_test, knn_prediction)\n",
    "\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a67347d",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'n_neighbors': [5,10,15,20],\n",
    "                  'weights': ['uniform', 'distance'],\n",
    "                  'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           scoring='precision_weighted',\n",
    "                           cv=10,\n",
    "                           n_jobs=3,\n",
    "                           verbose=4)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ad1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = grid_search.best_estimator_\n",
    "yk_pred = knn.predict(X_test)\n",
    "\n",
    "best_knn_classification_report = classification_report(y_test, yk_pred, output_dict=True)\n",
    "\n",
    "print(\"--- Improved model ---\\n\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, yk_pred)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_knn = confusion_matrix(y_test, yk_pred)\n",
    "\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bfa3e",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Support-Vector Machines <a class=\"anchor\" id=\"support-vector-machines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76062992",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "svc_prediction = svc.predict(X_test)\n",
    "\n",
    "svm_classification_report = classification_report(y_test, svc_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Classification report:\\n{classification_report(y_test, svc_prediction)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_svm = confusion_matrix(y_test, svc_prediction)\n",
    "\n",
    "sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "best_svm_classification_report = svm_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127e0df9",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce014bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'C' : [0.1, 1, 10], \n",
    "                'kernel': ['linear', 'poly', 'rbf', 'sigmoid']}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(),\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=10,\n",
    "                           verbose=4,\n",
    "                           n_jobs=4)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Best score: {grid_search.best_score_}\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best estimator: {grid_search.best_estimator_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b75909",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = grid_search.best_estimator_\n",
    "yk_pred = svc.predict(X_test)\n",
    "\n",
    "best_svm_classification_report = classification_report(y_test, yk_pred, output_dict=True)\n",
    "\n",
    "print(\"--- Improved model ---\\n\")\n",
    "print(f\"Classification report:\\n{best_knn_classification_report(y_test, yk_pred)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_svm = confusion_matrix(y_test, yk_pred)\n",
    "\n",
    "sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f9a3a0",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Neural Networks <a class=\"anchor\" id=\"neural-networks\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc05d281",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train_nn = scaler.transform(X_train)\n",
    "X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "# Create the classifier\n",
    "ANNClassifier = MLPClassifier(random_state=1, max_iter=500)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "ANNClassifier.fit(X_train_nn, y_train)\n",
    "\n",
    "predictions = ANNClassifier.predict(X_test_nn)\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test,predictions)\n",
    "\n",
    "nn_classification_report = classification_report(y_test, predictions, output_dict=True)\n",
    "print(classification_report(y_test,predictions))\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "best_nn_classification_report = nn_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc1246",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f04132",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'activation': ['tanh','identity','logistic','relu'],\n",
    "                  'solver': ['adam','lbfgs','sgd'],\n",
    "                  'hidden_layer_sizes': [3,5,8,13,21,34],\n",
    "                  'verbose': [True]}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "grid_search = GridSearchCV(ANNClassifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d4107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNClassifier = grid_search.best_estimator_\n",
    "yk_pred = ANNClassifier.predict(X_test)\n",
    "\n",
    "best_nn_classification_report = classification_report(y_test, yk_pred, output_dict=True)\n",
    "\n",
    "print(\"--- Improved model ---\\n\")\n",
    "print(f\"Classification report:\\n{best_nn_classification_report(y_test, yk_pred)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test, yk_pred)\n",
    "\n",
    "sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
