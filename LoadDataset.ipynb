{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TO-DO**\n",
    "* assinalar que owners é que têm um disponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents <a id=\"index\"></a>\n",
    "\n",
    "\n",
    "- [Dataset managing](#dataset)  \n",
    "- [District Data](#district-data)\n",
    "- [Data Exploration](#data-exploration)\n",
    "- [Matrix](#matrix)\n",
    "  \n",
    "\n",
    "#### Models\n",
    "- [**Decision Tree**](#decision-tree)\n",
    "    - [**Parameter Tunning**](#parameter-tunning)\n",
    "- [**K-Nearest Neighbor**](#k-nearest-neighbor)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-2)  \n",
    "- [**Support-Vector Machines**](#support-vector-machines)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-3)\n",
    "- [**Neural Networks**](#neural-networks)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-4)\n",
    "- [**Logistic Regression**](#logistic-regression)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-5)\n",
    "- [**Naive Bayes**](#naive-bayes)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-6)\n",
    "- [**Random Forest**](#random-forest)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-7)\n",
    "- [**XGBoost**](#xgboost)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-8)\n",
    "- [**MLP**](#mlp)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-9)\n",
    "- [**Ada Boost**](#ada-boost)\n",
    "    - [**Parameter Tunning**](#parameter-tunning-10)\n",
    "- [**Voting**](#voting)\n",
    "\n",
    "#### [Apply Model](#apply-model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(x, missing = \"\"):\n",
    "    return pd.read_csv('Dataset/' + x + '.csv', sep = ';', low_memory = False, na_values = missing_values).rename(str.strip, axis = 'columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting number of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_credit_cash(val):\n",
    "    return sum(val==\"credit in cash\")\n",
    "def count_collect(val):\n",
    "    return sum(val==\"collection from another bank\")\n",
    "def count_with_cash(val):\n",
    "    return sum(val==\"withdrawal in cash\")\n",
    "def count_remi(val):\n",
    "    return sum(val==\"remittance to another bank\")\n",
    "def count_with_card(val):\n",
    "    return sum(val==\"credit card withdrawal\")\n",
    "def count_interest(val):\n",
    "    return sum(val==\"interest credited\")\n",
    "\n",
    "def count_withdrawal(x):\n",
    "    return sum(x==\"withdrawal\")\n",
    "def count_credit(x):\n",
    "    return sum(x==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the mean of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_credit_cash(val):\n",
    "    return np.mean(val==\"credit in cash\")\n",
    "def mean_collect(val):\n",
    "    return np.mean(val==\"collection from another bank\")\n",
    "def mean_with_cash(val):\n",
    "    return np.mean(val==\"withdrawal in cash\")\n",
    "def mean_remi(val):\n",
    "    return np.mean(val==\"remittance to another bank\")\n",
    "def mean_with_card(val):\n",
    "    return np.mean(val==\"credit card withdrawal\")\n",
    "def mean_interest(val):\n",
    "    return np.mean(val==\"interest credited\")\n",
    "\n",
    "def mean_withdrawal(x):\n",
    "    return np.mean(x==\"withdrawal\")\n",
    "def mean_credit(x):\n",
    "    return np.mean(x==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the standard deviation of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_credit_cash(val):\n",
    "    return np.std(val==\"credit in cash\")\n",
    "def std_collect(val):\n",
    "    return np.std(val==\"collection from another bank\")\n",
    "def std_with_cash(val):\n",
    "    return np.std(val==\"withdrawal in cash\")\n",
    "def std_remi(val):\n",
    "    return np.std(val==\"remittance to another bank\")\n",
    "def std_with_card(val):\n",
    "    return np.std(val==\"credit card withdrawal\")\n",
    "def std_interest(val):\n",
    "    return np.std(val==\"interest credited\")\n",
    "\n",
    "\n",
    "def std_withdrawal(x):\n",
    "    return np.std(x==\"withdrawal\")\n",
    "def std_credit(x):\n",
    "    return np.std(x==\"credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for getting the covariance of the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov_credit_cash(val):\n",
    "    return np.cov(val==\"credit in cash\")\n",
    "def cov_collect(val):\n",
    "    return np.cov(val==\"collection from another bank\")\n",
    "def cov_with_cash(val):\n",
    "    return np.cov(val==\"withdrawal in cash\")\n",
    "def cov_remi(val):\n",
    "    return np.cov(val==\"remittance to another bank\")\n",
    "def cov_with_card(val):\n",
    "    return np.cov(val==\"credit card withdrawal\")\n",
    "def cov_interest(val):\n",
    "    return np.cov(val==\"interest credited\")\n",
    "\n",
    "def cov_withdrawal(x):\n",
    "    return np.cov(x==\"withdrawal\")\n",
    "def cov_credit(x):\n",
    "    return np.cov(x==\"credit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = ['?', 'NA', '']\n",
    "account_data = dataset('account', missing_values)\n",
    "client_data = dataset('client', missing_values)\n",
    "disp_data = dataset('disp', missing_values)\n",
    "district_data = dataset('district', missing_values)\n",
    "card_train = dataset('card_train', missing_values)\n",
    "card_test = dataset('card_test')\n",
    "loan_train = dataset('loan_train', missing_values)\n",
    "loan_test = dataset('loan_test')\n",
    "trans_train = dataset('trans_train', missing_values)\n",
    "trans_test = dataset('trans_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the distribution of the district regions for each one of the different regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sb.countplot(x='region', data=district_data)\n",
    "plt.title(\"Number of districts per region\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xlabel('Region');\n",
    "plt.ylabel('Number of districts from region');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To study if the region might have an impact on the acceptance of the loan we will create a graph comparing the percentages of successful loans per region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "district_data_region = district_data.copy()\n",
    "region_data = loan_train\n",
    "region_data = pd.merge(region_data, trans_train, on = 'account_id', suffixes = ('', '_trans'))\n",
    "region_data = pd.merge(region_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "#train_data = train_data.dropna()\n",
    "region_data = pd.merge(region_data, district_data_region.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "\n",
    "region_total = region_data[\"region\"].value_counts()\n",
    "tuples_total = [tuple((x, y)) for x, y in region_total.items()]\n",
    "\n",
    "regions_status_1 = region_data.loc[region_data['status'] == 1]\n",
    "region_total_1 = regions_status_1[\"region\"].value_counts()\n",
    "tuples_total_1 = [tuple((x, y)) for x, y in region_total_1.items()]\n",
    "\n",
    "lista=[]\n",
    "for x in tuples_total:\n",
    "    for y in tuples_total_1:\n",
    "        if x[0]==y[0]:\n",
    "            lista.append((x[0],x[1],y[1]))\n",
    "\n",
    "percentages = [(i[0],i[2] / i[1]) for i in lista]\n",
    "percentages.sort(key = lambda x: -x[1])\n",
    "\n",
    "x = [i[0] for i in percentages]\n",
    "y = [i[1] for i in percentages]\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sb.barplot(x,y)\n",
    "plt.ylim(0.7, 1)\n",
    "plt.title(\"Percentage of successful loans per region\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xlabel('Region');\n",
    "plt.ylabel('Percentage of successful loans');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see the different values the parameter type can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_data_pie = disp_data.copy()\n",
    "\n",
    "ser = disp_data_pie.groupby('type')['type'].count()\n",
    "ser = ser.sort_values(ascending=False)\n",
    "ser = ser.iloc[[0,1]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pie(ser.values, labels=ser.index, startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "ax.legend()\n",
    "plt.axis('equal')\n",
    "plt.title('Type of disposition', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since only owner can issue permanent orders and ask for a loan, there is no interess in keeping this parameter in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing name and region from district\n",
    "district_data.drop(['name', 'region'], axis=1, inplace=True)\n",
    "\n",
    "# only owner can issue permanent orders and ask for a loan\n",
    "disp_owners = disp_data[disp_data.type.eq('OWNER')]\n",
    "disp_owners.drop(['type'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As *birth_number* column doesn't add any value to our model as it is in the original data (it's represented as an int in format YYMMDD for men and YYMM+50DD for women), let's put it in format YY-MM-DD and create a new column called *genre*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize birthday dates and add a new column (Genre) to distinguish\n",
    "\n",
    "birth_dates = client_data['birth_number']\n",
    "dates_parsed = []\n",
    "genre = []\n",
    "for date in birth_dates:\n",
    "    month = int(str(date)[2:4])\n",
    "    if month > 12:\n",
    "        genre.append(0)\n",
    "        month = month - 50\n",
    "        if month < 10:\n",
    "            month = '0' + str(month)\n",
    "        else:\n",
    "            month = str(month)\n",
    "    else:\n",
    "        #print('AAAA: ' + str(month))\n",
    "        if month < 10:\n",
    "            month = '0' + str(month)\n",
    "            #print('BBBB: ' + str(month))\n",
    "        else:\n",
    "            month = str(month)\n",
    "        genre.append(1)\n",
    "    dates_parsed.append(str(date)[:2] + '-' + month + '-' + str(date)[4:])\n",
    "    \n",
    "\n",
    "# client_data = client_data.drop(['birth_number'], axis = 1)\n",
    "client_data['birth_number'] = dates_parsed\n",
    "client_data['genre'] = genre\n",
    "client_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the *genre* distribution, where 1 means male and 0 female."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_data_genre_copy = client_data.copy()\n",
    "\n",
    "ser = client_data_genre_copy.groupby('genre')['genre'].count()\n",
    "ser = ser.sort_values(ascending=False)\n",
    "ser = ser.iloc[[0,1]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pie(ser.values, labels=[\"Male\",\"Female\"], startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "ax.legend()\n",
    "plt.axis('equal')\n",
    "plt.title('Genre distribution', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train dataset\n",
    "\n",
    "train_data = loan_train\n",
    "train_data = pd.merge(train_data, trans_train, on = 'account_id', suffixes = ('', '_trans'))\n",
    "train_data = pd.merge(train_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "#train_data = train_data.dropna()\n",
    "train_data = pd.merge(train_data, district_data.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "train_data = pd.merge(train_data, disp_owners, on = 'account_id', suffixes = ('', '_disp'))\n",
    "train_data = pd.merge(train_data, card_train, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "train_data = pd.merge(train_data, client_data, on = 'client_id', suffixes = ('', '_client'))\n",
    "train_data = train_data.drop(['district_id_client'], axis=1)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build test dataset\n",
    "\n",
    "test_data = loan_test\n",
    "test_data = pd.merge(test_data, trans_test, on = 'account_id', suffixes = ('', '_trans'))\n",
    "test_data = pd.merge(test_data, account_data, on = 'account_id', suffixes = ('', '_account'))\n",
    "test_data = pd.merge(test_data, district_data.set_index('code'), left_on = 'district_id', right_index = True, suffixes = ('', '_district'))\n",
    "test_data = pd.merge(test_data, disp_owners, on = 'account_id', suffixes = ('', '_disp'))\n",
    "test_data = pd.merge(test_data, card_test, on = 'disp_id', how = 'outer', suffixes = ('', '_card'))\n",
    "test_data = pd.merge(test_data, client_data, on = 'client_id', suffixes = ('', '_client'))\n",
    "test_data = test_data.drop(['district_id_client'], axis=1)\n",
    "test_data.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a column *age_loan* where we will keep the age of the client at the time of the requested loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "#print(len(birth_dates))\n",
    "#print(sorted(birth_dates))\n",
    " \n",
    "def calculateAge(birthDate, loanDate):\n",
    "    loan = loanDate.split(\"-\")\n",
    "    loan_date = date(int(loan[0]), int(loan[1]), int(loan[2]))\n",
    "    birth = birthDate.split(\"-\")\n",
    "    \n",
    "    birth_date = date(int(birth[0]), int(birth[1]), int(birth[2]))\n",
    "    return (int((loan_date-birth_date).days/365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dates = train_data['date']\n",
    "\n",
    "loan_dates = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_dates]\n",
    "\n",
    "birth_dates = train_data['birth_number']\n",
    "\n",
    "#print(sorted(birth_dates))\n",
    "\n",
    "age_list = [calculateAge(birth_dates[n],loan_dates[n]) for n in range(0,len(birth_dates))]\n",
    "\n",
    "train_data[\"age_loan\"] = age_list\n",
    "\n",
    "# print(train_data[\"age_loan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_dates = test_data['date']\n",
    "\n",
    "loan_dates = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in loan_dates]\n",
    "\n",
    "birth_dates = test_data['birth_number']\n",
    "\n",
    "age_list = [calculateAge(birth_dates[n],loan_dates[n]) for n in range(0,len(test_data))]\n",
    "\n",
    "test_data[\"age_loan\"] = age_list\n",
    "\n",
    "# print(test_data[\"age_loan\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's create a new column that calculates the age of each client when he creates the account (*age_account*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_dates_train = train_data['date_account']\n",
    "account_dates_test = test_data['date_account']\n",
    "\n",
    "account_dates_train = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in account_dates_train]\n",
    "account_dates_test = [str(int(x))[:2] + \"-\" + str(int(x))[2:4] + \"-\" + str(int(x))[4:] for x in account_dates_test]\n",
    "\n",
    "age_list_train = [calculateAge(birth_dates[n],account_dates_train[n]) for n in range(0,len(train_data))]\n",
    "age_list_test = [calculateAge(birth_dates[n],account_dates_test[n]) for n in range(0,len(test_data))]\n",
    "\n",
    "train_data['age_account'] = age_list_train\n",
    "test_data['age_account'] = age_list_test\n",
    "\n",
    "# print(train_data[\"age_account\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's simplify *age_loan* and *age_account* values, converting them into age ranges divided by decades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decade(year):\n",
    "    year_int = int(year)\n",
    "    string = str(year_int//10) + \"0-\" + str(year_int//10) + \"9\"\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_loan_train = train_data['age_loan'].astype(str)\n",
    "age_loan_test = test_data['age_loan'].astype(str)\n",
    "\n",
    "age_loan_train = [get_decade(age_loan_train[n]) for n in range(0, len(train_data))]\n",
    "age_loan_test = [get_decade(age_loan_test[n]) for n in range(0, len(test_data))]\n",
    "\n",
    "train_data['age_loan_range'] = age_loan_train\n",
    "test_data['age_loan_range'] = age_loan_test\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_loan_train = train_data['age_account'].astype(str)\n",
    "age_loan_test = test_data['age_account'].astype(str)\n",
    "\n",
    "age_loan_train = [get_decade(age_loan_train[n]) for n in range(0, len(train_data))]\n",
    "age_loan_test = [get_decade(age_loan_test[n]) for n in range(0, len(test_data))]\n",
    "\n",
    "train_data['age_account_range'] = age_loan_train\n",
    "test_data['age_account_range'] = age_loan_test\n",
    "\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "\n",
    "train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_copy = train_data_copy.sort_values('age_loan')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.title(\"Age by decade at the date of the loan\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "ax = sb.countplot(x=\"age_loan\", data=train_data_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the months when the accounts were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month(year):\n",
    "    string = year[2:4] + \"/\" + year[0:2]\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.sort_values('date_account')\n",
    "train_data_copy = train_data_copy.astype({'date_account': str})\n",
    "\n",
    "train_data_copy['date_account'] = train_data_copy['date_account'].apply(lambda x: get_month(x[0:4]))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(25,20))\n",
    "plt.title(\"Date of creation of the account per month\")\n",
    "ax = sb.countplot(x=\"date_account\", data=train_data_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the month and year of when the loans were created and their relation to the number of successful loans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.sort_values('date')\n",
    "train_data_copy = train_data_copy.astype({'date': str})\n",
    "\n",
    "train_data_copy['date'] = train_data_copy['date'].apply(lambda x: get_month(x[0:4]))\n",
    "\n",
    "train_data_copy = train_data_copy.astype({'status': str})\n",
    "train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Successful Loan','Unsuccessful Loan'])\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(25,20))\n",
    "plt.title(\"Number of successful loans per month\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "ax = sb.countplot(x ='date', hue = \"status\", data = train_data_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows we can see the months when the most transfers were taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.sort_values('date_trans')\n",
    "train_data_copy = train_data_copy.astype({'date_trans': str})\n",
    "\n",
    "train_data_copy['date_trans'] = train_data_copy['date_trans'].apply(lambda x: get_month(x[0:4]))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.figure(figsize=(25,20))\n",
    "plt.title(\"Months with the most tranfers\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "ax = sb.countplot(x=\"date_trans\", data=train_data_copy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken all informations that we need from all dates, we can remove these columns from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['date', 'date_trans', 'date_account', 'birth_number'], axis=1, inplace=True)\n",
    "test_data.drop(['date', 'date_trans', 'date_account', 'birth_number'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values for each attribute\n",
    "train_data.isnull().sum().plot(kind='bar', figsize=(18,8), fontsize=14,);\n",
    "plt.title(\"Number of null variables per collumn\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Null values');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove *bank* and *account* columns, as these columns only record the destination account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(['bank', 'account'], axis=1, inplace=True)\n",
    "test_data.drop(['bank', 'account'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the possible values and the respective frequency for some columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.operation.value_counts())\n",
    "print('Null values: ' + str(train_data.operation.isnull().sum()))\n",
    "print()\n",
    "print(train_data.k_symbol.value_counts())\n",
    "print('Null values: ' + str(train_data.k_symbol.isnull().sum()))\n",
    "print()\n",
    "print(train_data.type_card.value_counts())\n",
    "print('Null values: ' + str(train_data.type_card.isnull().sum()))\n",
    "print()\n",
    "print(train_data.type.value_counts())\n",
    "print('Null values: ' + str(train_data.type.isnull().sum()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy = train_data_copy.loc[:, train_data_copy.columns.intersection(['type','operation'])]\n",
    "train_data_copy[\"operation\"] = train_data_copy[\"operation\"].fillna('NaN')\n",
    "\n",
    "train_data_copy.sort_values(\"operation\")\n",
    "\n",
    "print(train_data_copy[\"type\"].unique())\n",
    "print(train_data_copy[\"operation\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.displot(train_data_copy,x='operation', y='type', aspect=3)\n",
    "plt.xlabel('Operation');\n",
    "plt.ylabel('Type');\n",
    "plt.title(\"Comparition between Operation and Type\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the rows with operation *credit in cash* and *collection from another bank* are of type credit. All the *withdrawal in cash*, *remittance to another bank* and *credit cash withdrawal* operations are of the type *withdraw* or *withdrawal with cash* (in the case of the homonymous operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "print(train_data_copy.operation.value_counts())\n",
    "train_data_copy = train_data_copy.groupby(['k_symbol', 'operation'])['operation'].count()\n",
    "print(train_data_copy)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "train_data_copy.plot(kind='bar', stacked=True)\n",
    "plt.xlabel('K_symbol');\n",
    "plt.ylabel('Quantity');\n",
    "plt.title(\"Number of operation types per k_symbol\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy[\"operation\"]=train_data_copy[\"operation\"].fillna(\"Not Available\")\n",
    "train_data_copy[\"k_symbol\"]=train_data_copy[\"k_symbol\"].fillna(\"Not Available\")\n",
    "train_data_copy['k_symbol'] = train_data_copy['k_symbol'].replace([' '],['Not Available'])\n",
    "train_data_copy['operation'] = train_data_copy['operation'].replace([''],['Not Available'])\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "sb.catplot(x=\"operation\", hue=\"k_symbol\", kind=\"count\",\n",
    "            palette=\"pastel\", edgecolor=\".6\",\n",
    "            data=train_data_copy, height=10, aspect=1.5)\n",
    "plt.title('Relation between k_symbols and operation', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylim(0)\n",
    "plt.rcParams['figure.figsize']=(20,10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the great majority of the k_symbol types are directly related to a certain operation type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for **type** column, let's convert all *withdrawal in cash* occurrences into *withdrawal*, as it is basically the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['type'] == \"withdrawal in cash\", 'type'] = \"withdrawal\"\n",
    "test_data.loc[test_data['type'] == \"withdrawal in cash\", 'type'] = \"withdrawal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, looking at *operation* and *k_symbol* columns, let's select all rows that have null value in *operation* and look at the respective value in *k_symbol* value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operations_null = train_data[train_data['operation'].isnull()]\n",
    "operations_null.k_symbol.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All rows that have **operation** column with null value have *interested credited* in *k_symbol* column. We can try to replace the null values of *operation* column with the value that is in *k_symbol* column and then delete *k_symbol* parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.loc[train_data['operation'].isna(), 'operation'] = train_data.loc[train_data['operation'].isna(), 'k_symbol']\n",
    "test_data.loc[test_data['operation'].isna(), 'operation'] = test_data.loc[test_data['operation'].isna(), 'k_symbol']\n",
    "\n",
    "train_data.drop(['k_symbol'], axis=1, inplace=True)\n",
    "test_data.drop(['k_symbol'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the Card Type, with NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy['type_card'] = train_data_copy['type_card'].fillna(\"NaN\")\n",
    "\n",
    "ser = train_data_copy.groupby('type_card')['type_card'].count()\n",
    "ser = ser.sort_values(ascending=False)\n",
    "ser = ser.iloc[[0,1,2,3]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pie(ser.values, labels=['','','',''], startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "ax.legend(loc=3, labels=ser.index)\n",
    "plt.axis('equal')\n",
    "plt.title('Card Type distribution', y=1.05, fontsize=20)\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the Card Type, without NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "ser = train_data_copy.groupby('type_card')['type_card'].count()\n",
    "ser = ser.sort_values(ascending=False)\n",
    "ser = ser.iloc[[0,1,2]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pie(ser.values, labels=ser.index, startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "ax.legend()\n",
    "plt.axis('equal')\n",
    "plt.title('Card Type distribution', y=1.05, fontsize=20)\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy['type_card'] = train_data_copy['type_card'].fillna(\"NaN\")\n",
    "\n",
    "ser = train_data_copy.groupby('type_card')['type_card'].count()\n",
    "ser = ser.sort_values(ascending=False)\n",
    "ser = ser.iloc[[0,1,2,3]]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.pie(ser.values, labels=['','','',''], startangle=90, autopct=lambda x:int(x/100.*ser.sum()+0.1), pctdistance=0.8, counterclock=False)\n",
    "ax.legend(loc=3, labels=ser.index)\n",
    "plt.axis('equal')\n",
    "plt.title('Card Type distribution', y=1.05, fontsize=20)\n",
    "fig.set_size_inches(15, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### District Data <a id=\"district-data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "district_scatter_plot = sb.PairGrid(district_data)\n",
    "district_scatter_plot.map(plt.scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null values unemploymant rate in \\'95:' + str(district_data['unemploymant rate \\'95'].isnull().sum()))\n",
    "print()\n",
    "print('Null values no. of commited crimes \\'95 :' + str(district_data['no. of commited crimes \\'95'].isnull().sum()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Distribution of district\\'s unemploymant rate in \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.hist(district_data['unemploymant rate \\'95'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Representation of the Card Type, without NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Distribution of district\\'s no. of commited crimes \\'95', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.hist(district_data['no. of commited crimes \\'95'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill null values in district's unemploymant rate in '95 and district's no. of commited crimes '95\n",
    "# See if it is better to use median or mean \n",
    "\n",
    "train_data['unemploymant rate \\'95'].fillna(train_data['unemploymant rate \\'95'].median(), inplace=True)\n",
    "\n",
    "train_data['no. of commited crimes \\'95'].fillna(train_data['no. of commited crimes \\'95'].mean(), inplace=True)\n",
    "\n",
    "test_data['unemploymant rate \\'95'].fillna(test_data['unemploymant rate \\'95'].median(), inplace=True)\n",
    "\n",
    "test_data['no. of commited crimes \\'95'].fillna(test_data['no. of commited crimes \\'95'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### Dataset Managing <a id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs to be checked\n",
    "option = 0\n",
    "if option == 0:\n",
    "    train_data.drop(['issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['issued'], axis=1, inplace=True)\n",
    "if option == 1:\n",
    "    train_data.drop(['operation', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['operation', 'issued'], axis=1, inplace=True)\n",
    "# dropped type and issued\n",
    "elif option == 2:\n",
    "    train_data.drop(['issued', 'type'], axis=1, inplace=True)\n",
    "    test_data.drop(['issued', 'type'], axis=1, inplace=True)\n",
    "#doesn't work\n",
    "elif option == 4:\n",
    "    train_data['operation'].fillna(train_data['k_symbol'], inplace=True)\n",
    "    test_data['operation'].fillna(test_data['k_symbol'], inplace=True)\n",
    "    columns = train_data.columns\n",
    "    print(columns)\n",
    "    train_data=train_data.agg({\n",
    "        \"type\": [count_withdrawal, count_credit, mean_withdrawal, mean_credit, std_withdrawal, std_credit, cov_withdrawal, cov_credit],\n",
    "        \"operation\":[\"count\", count_credit_cash, count_collect, count_with_cash, count_remi, count_with_card, count_interest, \n",
    "                     mean_credit_cash, mean_collect, mean_with_cash, mean_remi, mean_with_card, mean_interest,\n",
    "                     std_credit_cash, std_collect, std_with_cash, std_remi, std_with_card, std_interest,\n",
    "                     cov_credit_cash, cov_collect, cov_with_cash, cov_remi, cov_with_card, cov_interest]\n",
    "        \n",
    "    })\n",
    "    test_data=test_data.agg({\n",
    "        \"type\": [count_withdrawal, count_credit, mean_withdrawal, mean_credit, std_withdrawal, std_credit, cov_withdrawal, cov_credit],\n",
    "        \"operation\":[\"count\", count_credit_cash, count_collect, count_with_cash, count_remi, count_with_card, count_interest, \n",
    "                     mean_credit_cash, mean_collect, mean_with_cash, mean_remi, mean_with_card, mean_interest,\n",
    "                     std_credit_cash, std_collect, std_with_cash, std_remi, std_with_card, std_interest,\n",
    "                     cov_credit_cash, cov_collect, cov_with_cash, cov_remi, cov_with_card, cov_interest]\n",
    "        \n",
    "    })\n",
    "    train_data.drop(['k_symbol', 'issued'], axis=1, inplace=True)\n",
    "    test_data.drop(['k_symbol', 'issued'], axis=1, inplace=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.get_dummies(train_data, columns=['frequency'], dtype=bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['frequency'], dtype=bool)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['type_card'], dtype = bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['type_card'], dtype = bool)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['age_loan_range'], dtype = bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['age_loan_range'], dtype = bool)\n",
    "\n",
    "train_data = pd.get_dummies(train_data, columns=['age_account_range'], dtype = bool)\n",
    "test_data = pd.get_dummies(test_data, columns=['age_account_range'], dtype = bool)\n",
    "\n",
    "if option != 2:\n",
    "    train_data = pd.get_dummies(train_data, columns=['type'], dtype = bool)\n",
    "    test_data = pd.get_dummies(test_data, columns=['type'], dtype = bool)\n",
    "\n",
    "if option != 1 :\n",
    "    train_data = pd.get_dummies(train_data, columns=['operation'], dtype = bool)\n",
    "    test_data = pd.get_dummies(test_data, columns=['operation'], dtype = bool)\n",
    "\n",
    "test_data = test_data.drop_duplicates(subset=['loan_id'], keep='first')\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competition_inputs = test_data.drop(columns=[\"loan_id\", \"status\"])\n",
    "# test_data = test_data.drop(columns=[\"status\"])\n",
    "all_ids_comp = test_data['loan_id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "#### Data Exploration <a class=\"anchor\" id=\"data-exploration\"></a>\n",
    "\n",
    "After the previous changes we end up with the following columns to study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "ax = sb.barplot(train_data[\"duration\"], train_data[\"amount\"])\n",
    "plt.xlabel('Duration');\n",
    "plt.ylabel('Amount');\n",
    "plt.title('Loan duration compared the amount', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that with bigger loan amounts the time taken to repay them is exponentially higher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "ax = plt.plot(train_data[\"amount\"], train_data[\"payments\"], linestyle='none', marker='o', alpha=0.3)\n",
    "plt.title('Number of payments per different amount', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.xlabel('Amount');\n",
    "plt.ylabel('Payments');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can check by the values in the graph, it seems that the number of payments is defined by the bank since the results are very linear and follow a line according to the amount loaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "#ax = sb.barplot(train_data[\"amount_trans\"], train_data[\"balance\"])\n",
    "plt.plot(train_data[\"balance\"],train_data[\"amount_trans\"], 'o', color='green');\n",
    "plt.title('Distribution of tranfer amount by balance', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylabel('Transfer Amount');\n",
    "plt.xlabel('Balance');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see as the balance increases the transfer amount also increase and is rarely higher than the balance. It can also be seen that a straight continous line exists with gradient 1. This means the amount of some transfers are actually correspondent, or close, to the total amount in the account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.astype({'status': str})\n",
    "train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Successful Loan','Unsuccessful Loan'])\n",
    "\n",
    "train_data_copy = train_data_copy.groupby(['no. of cities', 'status'])['status'].count().unstack().fillna(0)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "train_data_copy.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Number of cities per district and loan success rate\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy = train_data_copy.astype({'status': str})\n",
    "train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Successful Loan','Unsuccessful Loan'])\n",
    "\n",
    "fig = plt.figure()\n",
    "sb.violinplot(y=train_data_copy[\"no. of inhabitants\"], x=train_data_copy[\"status\"])\n",
    "plt.title('Influence of number of inhabitants in loan success', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.ylim(0)\n",
    "fig.set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sorted(train_data[\"no. of inhabitants\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "\n",
    "train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_copy = train_data_copy.astype({'status': str})\n",
    "train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Successful Loan','Unsuccessful Loan'])\n",
    "\n",
    "train_data_copy = train_data_copy.groupby(['age_loan', 'status'])['status'].count().unstack().fillna(0)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "train_data_copy.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Loan success rate per decade of age\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "\n",
    "train_data_copy = train_data_copy.astype({'status': str})\n",
    "train_data_copy['status'] = train_data_copy['status'].replace(['1.0','-1.0'],['Successful Loan','Unsuccessful Loan'])\n",
    "\n",
    "train_data_copy = train_data_copy.groupby(['genre', 'status'])['status'].count().unstack().fillna(0)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (15,10)\n",
    "train_data_copy.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Loan success rate per decade of genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "train_data_names = train_data.copy()\n",
    "train_data_names[\"genre\"] = train_data_names[\"genre\"].astype(str)\n",
    "train_data_names[\"genre\"].replace({\"1\": \"Male\" , \"0\": \"Female\"}, inplace=True)\n",
    "\n",
    "sb.scatterplot(train_data_names[\"amount\"],train_data_names[\"age_loan\"],train_data_names[\"genre\"], alpha=0.5, sizes=(10, 1000), hue=\"time\")\n",
    "plt.xlabel(\"Amount\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.title(\"Amount by age and genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decade(year):\n",
    "    year_int = int(year)\n",
    "    string = str(year_int//10) + \"0-\" + str(year_int//10) + \"9\"\n",
    "    return string\n",
    "\n",
    "train_data_names = train_data.copy()\n",
    "\n",
    "train_data_names = train_data_names.astype({'age_loan': str})\n",
    "\n",
    "train_data_names['age_loan'] = train_data_names['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_names = train_data_names.sort_values('age_loan')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "train_data_names[\"genre\"] = train_data_names[\"genre\"].astype(str)\n",
    "train_data_names[\"genre\"].replace({\"1\": \"Male\" , \"0\": \"Female\"}, inplace=True)\n",
    "\n",
    "sb.boxplot(x=train_data_copy[\"age_loan\"], y=train_data_copy[\"amount\"])\n",
    "plt.title(\"Amount by age and genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decade(year):\n",
    "    year_int = int(year)\n",
    "    string = str(year_int//10) + \"0-\" + str(year_int//10) + \"9\"\n",
    "    return string\n",
    "\n",
    "train_data_names = train_data.copy()\n",
    "\n",
    "train_data_names = train_data_names.astype({'age_loan': str})\n",
    "\n",
    "train_data_names['age_loan'] = train_data_names['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_names = train_data_names.astype({'age_account': str})\n",
    "\n",
    "train_data_names['age_account'] = train_data_names['age_account'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_names = train_data_names.sort_values('age_loan')\n",
    "sb.barplot(x='age_account', y='amount', hue='age_loan', data=train_data_names, saturation=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decade(year):\n",
    "    year_int = int(year)\n",
    "    string = str(year_int//10) + \"0-\" + str(year_int//10) + \"9\"\n",
    "    return string\n",
    "\n",
    "def get_loan_amount(year):\n",
    "    year_int = int(float(year))\n",
    "    string = \"1\" + \"0\"*(len(str(year_int))-1) + \"-\" + \"9\" + \"9\"*(len(str(year_int))-1)\n",
    "    return string\n",
    "\n",
    "train_data_names = train_data.copy()\n",
    "\n",
    "train_data_names = train_data_names.astype({'age_loan': str})\n",
    "\n",
    "train_data_names['age_loan'] = train_data_names['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_names = train_data_names.astype({'balance': str})\n",
    "\n",
    "train_data_names['balance'] = train_data_names['balance'].apply(lambda x: get_loan_amount(x))\n",
    "\n",
    "train_data_names = train_data_names.sort_values('balance')\n",
    "sb.boxplot(x='balance', y='amount', hue='age_loan', data=train_data_names, saturation=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "train_data_names = train_data.copy()\n",
    "train_data_names[\"genre\"] = train_data_names[\"genre\"].astype(str)\n",
    "train_data_names[\"genre\"].replace({\"1\": \"Male\" , \"0\": \"Female\"}, inplace=True)\n",
    "\n",
    "sb.scatterplot(train_data_names[\"average salary\"],train_data_names[\"age_loan\"],train_data_names[\"district_id\"], alpha=0.5, sizes=(10, 1000), hue=\"time\")\n",
    "plt.xlabel(\"Average Salary\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.title(\"Average Salary by age and district\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As follows we can see the average salary by age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy = train_data_copy.astype({'age_loan': str})\n",
    "\n",
    "train_data_copy['age_loan'] = train_data_copy['age_loan'].apply(lambda x: get_decade(x))\n",
    "\n",
    "train_data_copy = train_data_copy.sort_values('age_loan')\n",
    "train_data_copy[\"average salary\"] = train_data_copy[\"average salary\"].astype(float)\n",
    "\n",
    "fig = plt.figure()\n",
    "sb.boxplot(x=train_data_copy[\"age_loan\"], y=train_data_copy[\"average salary\"])\n",
    "plt.xlabel('age_loan', y=1.05, fontsize=15, labelpad=15)\n",
    "plt.ylabel('average salary', x=0.7, fontsize=15, labelpad=15)\n",
    "plt.title('Average salary per decade of age', fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "fig.set_size_inches(15, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_copy = train_data.copy()\n",
    "train_data_copy[\"genre\"] = train_data_copy[\"genre\"].astype(str)\n",
    "train_data_copy[\"genre\"].replace({\"0\": \"Female\",\"1\": \"Male\"}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.ylim(9000, 10000)\n",
    "sb.barplot(x = train_data_copy[\"genre\"], y = train_data_copy[\"average salary\"])\n",
    "plt.title(\"Average Salary by genre\", fontdict={'fontsize': 14, 'fontweight': 'bold'})\n",
    "# display\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[back](#index)\n",
    "#### Matrix <a id=\"matrix\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data_w /test_data_w : train/test data where withdrawals in cash are replaced by withdrawals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ids\n",
    "#train_data_no_ids = train_data.drop(['loan_id', 'account_id', 'district_id', 'disp_id', 'client_id', 'card_id', 'trans_id'], axis=1)\n",
    "train_data_no_ids = train_data.drop(['client_id', 'account_id', 'district_id', 'disp_id', 'card_id', 'trans_id', 'age_loan', 'age_account'], axis=1)\n",
    "test_data_no_ids = test_data.drop(['client_id', 'account_id', 'district_id', 'disp_id', 'card_id', 'trans_id', 'age_loan', 'age_account'], axis=1)\n",
    "\n",
    "\n",
    "# Create correlation matrix\n",
    "corr_matrix = train_data_no_ids.corr().abs()\n",
    "plt.figure(figsize = (20,6))\n",
    "sb.heatmap(corr_matrix, annot=True)\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find features with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "train_data_no_ids.drop(to_drop, axis=1, inplace=True)\n",
    "test_data_no_ids.drop(to_drop, axis=1, inplace=True)\n",
    "\n",
    "print(\"{} Dropped columns: {}\".format(len(to_drop), to_drop) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with nan values for status\n",
    "#train_data_no_ids.dropna(subset=[\"status\"], inplace=True)\n",
    "\n",
    "#print(no_ids.drop_duplicates(inplace=True)\n",
    "print(train_data_no_ids[\"status\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_no_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = train_data_no_ids[train_data_no_ids.columns.drop(['loan_id', 'status'])]\n",
    "all_labels = train_data_no_ids['status'].values\n",
    "\n",
    "# competition_inputs = test_data_no_ids.drop(columns=[\"loan_id\"])\n",
    "# all_ids_comp = test_data_no_ids['loan_id'].values\n",
    "\n",
    "#all_inputs = train_data_no_ids.iloc[:, :-1].values\n",
    "#all_labels = train_data_no_ids.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a test dataset with 25% of the credit_data_subset\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(all_inputs, all_labels, test_size=0.25, random_state=1)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Classifiers <a class=\"anchor\" id=\"classifiers\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 5\n",
    "\n",
    "if model == 0:\n",
    "    classifier = DecisionTreeClassifier()\n",
    "elif model == 1:\n",
    "    classifier = KNeighborsClassifier()\n",
    "elif model == 2:\n",
    "    classifier = SVC(probability=True)\n",
    "elif model == 3:\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit only to the training data\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # Now apply the transformations to the data:\n",
    "    X_train_nn = scaler.transform(X_train)\n",
    "    X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "    # Create the classifier\n",
    "    classifier = MLPClassifier(random_state=1, max_iter=500) \n",
    "elif model == 4:\n",
    "    classifier = LogisticRegression()\n",
    "elif model == 5:\n",
    "    classifier = GaussianNB()\n",
    "elif model == 6:\n",
    "    classifier = RandomForestClassifier(300)\n",
    "elif model == 7:\n",
    "    classifier = XGBClassifier()\n",
    "elif model == 8:\n",
    "    classifier = MLPClassifier(alpha=1, max_iter=1000)\n",
    "elif model == 9:\n",
    "    classifier = AdaBoostClassifier()\n",
    "elif model == 10:\n",
    "    classifier = VotingClassifier(\n",
    "     estimators=[('dt', DecisionTreeClassifier()), ('svm', LinearSVC()), ('xgb', XGBClassifier())],\n",
    "     voting='hard', weights=[1,1,1]\n",
    ")\n",
    "    \n",
    "classifier.fit(X_train, y_train)\n",
    "classifier_prediction = classifier.predict(X_test)\n",
    "\n",
    "classifier_classification_report = classification_report(y_test, classifier_prediction, output_dict=True)\n",
    "\n",
    "print(f\"Classification report:\\n{classification_report(y_test, classifier_prediction)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    " \n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_dtc = confusion_matrix(y_test, classifier_prediction)\n",
    "\n",
    "sb.heatmap(confusion_matrix_classifier, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Decision Tree <a class=\"anchor\" id=\"decision-tree\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training set\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "dtc_prediction = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "dtc_classification_report = classification_report(y_test, dtc_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = decision_tree_classifier.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = decision_tree_classifier.predict(X_test) \n",
    "f1_dt = f1_score(y_test, predictions_test)\n",
    "roc_dt = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_dt))\n",
    "print(f\"ROC: {roc_dt}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_grid = {'criterion': ['gini', 'entropy'],\n",
    "#                   'splitter': ['best', 'random'],\n",
    "#                   'max_depth': range(10, 20),\n",
    "#                   'max_features': range(10,20)}\n",
    "\n",
    "# grid_search = GridSearchCV(DecisionTreeClassifier(),\n",
    "#                            param_grid=parameter_grid,\n",
    "#                            cv=10,\n",
    "#                            verbose=4,\n",
    "#                            n_jobs=-1)\n",
    "\n",
    "dt_classifier = DecisionTreeClassifier(min_samples_leaf = 10)\n",
    "\n",
    "dt_grid_search = GridSearchCV(dt_classifier, scoring=\"precision_weighted\", cv=5, param_grid={})\n",
    "dt_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(dt_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = dt_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = dt_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_dt = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_dt, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## K-Nearest Neighbor <a class=\"anchor\" id=\"k-nearest-neighbor\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "knn_prediction = knn.predict(X_test)\n",
    "\n",
    "knn_classification_report = classification_report(y_test, knn_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = knn.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = knn.predict(X_test) \n",
    "f1_knn = f1_score(y_test, predictions_test)\n",
    "roc_knn = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_knn))\n",
    "print(f\"ROC: {roc_knn}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter_grid = {'n_neighbors': [5,10,15,20],\n",
    "#                   'weights': ['uniform', 'distance'],\n",
    "#                   'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "\n",
    "# grid_search = GridSearchCV(KNeighborsClassifier(),\n",
    "#                            param_grid=parameter_grid,\n",
    "#                            scoring='precision_weighted',\n",
    "#                            cv=10,\n",
    "#                            n_jobs=3,\n",
    "#                            verbose=4)\n",
    "\n",
    "knn_classifier = KNeighborsClassifier()\n",
    "\n",
    "parameter_grid = {'n_neighbors': list(range(1,30)),\n",
    "                'weights': ['uniform','distance'],\n",
    "                'p':[1,2]}\n",
    "\n",
    "knn_grid_search = GridSearchCV(knn_classifier, parameter_grid, scoring=\"roc_auc\", n_jobs=-1, cv=10)\n",
    "knn_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(knn_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = knn_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = knn_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_knn = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_knn, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Support-Vector Machines <a class=\"anchor\" id=\"support-vector-machines\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(probability=True)\n",
    "\n",
    "svc.fit(X_train, y_train)\n",
    "svc_prediction = svc.predict(X_test)\n",
    "\n",
    "svm_classification_report = classification_report(y_test, svc_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = svc.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svc = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_svc, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = svc.predict(X_test) \n",
    "f1_svm = f1_score(y_test, predictions_test)\n",
    "roc_svm = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_svm))\n",
    "print(f\"ROC: {roc_svm}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svc = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_svc, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifier = SVC(probability=True)\n",
    "\n",
    "svm_grid_search = GridSearchCV(svm_classifier, scoring=\"roc_auc\", cv=5, param_grid={})\n",
    "svm_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(svm_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = svm_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svm = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = svm_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_svm = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_svm, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Neural Networks <a class=\"anchor\" id=\"neural-networks\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Now apply the transformations to the data:\n",
    "X_train_nn = scaler.transform(X_train)\n",
    "X_test_nn = scaler.transform(X_test)\n",
    "\n",
    "# Create the classifier\n",
    "ANNClassifier = MLPClassifier(random_state=1, max_iter=500)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "ANNClassifier.fit(X_train_nn, y_train)\n",
    "\n",
    "predictions_test = ANNClassifier.predict(X_test_nn)\n",
    "\n",
    "f1_nn = f1_score(y_test, predictions_test)\n",
    "roc_nn = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_nn))\n",
    "print(f\"ROC: {roc_nn}\")\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test,predictions_test)\n",
    "\n",
    "nn_classification_report = classification_report(y_test, predictions_test, output_dict=True)\n",
    "print(classification_report(y_test,predictions_test))\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "best_nn_classification_report = nn_classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = {'activation': ['tanh','identity','logistic','relu'],\n",
    "                  'solver': ['adam','lbfgs','sgd'],\n",
    "                  'hidden_layer_sizes': [3,5,8,13,21,34],\n",
    "                  'verbose': [True]}\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "grid_search = GridSearchCV(ANNClassifier,\n",
    "                           param_grid=parameter_grid,\n",
    "                           cv=cross_validation)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best score: {}'.format(grid_search.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search.best_params_))\n",
    "print('Best estimator: {}'.format(grid_search.best_estimator_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANNClassifier = grid_search.best_estimator_\n",
    "yk_pred = ANNClassifier.predict(X_test)\n",
    "\n",
    "best_nn_classification_report = classification_report(y_test, yk_pred, output_dict=True)\n",
    "\n",
    "print(\"--- Improved model ---\\n\")\n",
    "print(f\"Classification report:\\n{best_nn_classification_report(y_test, yk_pred)}\\n\")\n",
    "\n",
    "sb.set(font_scale=1.0)\n",
    "\n",
    "ax = plt.subplot()\n",
    "\n",
    "confusion_matrix_ann = confusion_matrix(y_test, yk_pred)\n",
    "\n",
    "sb.heatmap(confusion_matrix_ann, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Logistic Regression <a class=\"anchor\" id=\"logistic-regression\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr_prediction = lr.predict(X_test)\n",
    "\n",
    "lr_classification_report = classification_report(y_test, lr_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = lr.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = lr.predict(X_test) \n",
    "f1_lr = f1_score(y_test, predictions_test)\n",
    "roc_lr = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_lr))\n",
    "print(f\"ROC: {roc_lr}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier = LogisticRegression()\n",
    "\n",
    "lr_grid_search = GridSearchCV(lr_classifier, scoring=\"roc_auc\", cv=5, param_grid={})\n",
    "lr_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(lr_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = lr_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = lr_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_lr = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_lr, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Naive Bayes <a class=\"anchor\" id=\"naive-bayes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "nb_prediction = nb.predict(X_test)\n",
    "\n",
    "nb_classification_report = classification_report(y_test, nb_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = nb.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = nb.predict(X_test) \n",
    "f1_nb = f1_score(y_test, predictions_test)\n",
    "roc_nb = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_nb))\n",
    "print(f\"ROC: {roc_nb}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = GaussianNB()\n",
    "\n",
    "nb_grid_search = GridSearchCV(nb_classifier, scoring=\"roc_auc\", cv=5, param_grid={})\n",
    "nb_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(nb_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = nb_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = nb_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_nb = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_nb, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Random Forest <a class=\"anchor\" id=\"random-forest\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "rf_prediction = rf.predict(X_test)\n",
    "\n",
    "rf_classification_report = classification_report(y_test, rf_prediction, output_dict=True)\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = rf.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = rf.predict(X_test) \n",
    "f1_rf = f1_score(y_test, predictions_test)\n",
    "f1_roc = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_rf))\n",
    "print(f\"ROC: {f1_roc}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_grid_search = GridSearchCV(rf_classifier, scoring=\"roc_auc\", cv=5, param_grid={})\n",
    "rf_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(rf_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = rf_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = rf_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_rf = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_rf, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## XGBoost <a class=\"anchor\" id=\"xgboost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb = XGBClassifier()\n",
    "\n",
    "# xgb.fit(X_train, y_train)\n",
    "# xgb_prediction = xgb.predict(X_test)\n",
    "\n",
    "# xgb_classification_report = classification_report(y_test, xgb_prediction, output_dict=True)\n",
    "\n",
    "# print(f\"Classification report:\\n{classification_report(y_test, xgb_prediction, labels=np.unique(y_train))}\\n\")\n",
    "\n",
    "# sb.set(font_scale=1.0)\n",
    "\n",
    "# ax = plt.subplot()\n",
    "\n",
    "# confusion_matrix_xgb = confusion_matrix(y_test, xgb_prediction)\n",
    "\n",
    "# sb.heatmap(confusion_matrix_xgb, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "# ax.set_xlabel('Predicted Grades');\n",
    "# ax.set_ylabel('Observed Grades');\n",
    "# ax.set_title('Confusion Matrix');\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## MLP <a class=\"anchor\" id=\"mlp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "\n",
    "mlp.fit(X_train, y_train)\n",
    "mlp_prediction = mlp.predict(X_test)\n",
    "\n",
    "mlp_classification_report = classification_report(y_test, mlp_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = mlp.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_mlp = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_mlp, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = mlp.predict(X_test) \n",
    "f1_mlp = f1_score(y_test, predictions_test)\n",
    "roc_mlp = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_mlp))\n",
    "print(f\"ROC: {roc_mlp}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_mlp = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_mlp, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-9\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_classifier = MLPClassifier(random_state=1, early_stopping=False)\n",
    "\n",
    "mlp_grid_search = GridSearchCV(mlp_classifier, scoring=\"roc_auc\", cv=10, param_grid={})\n",
    "mlp_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(mlp_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = mlp_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_mlp = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_mlp, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = mlp_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_mlp = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_mlp, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Ada Boost <a class=\"anchor\" id=\"ada-boost\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = AdaBoostClassifier()\n",
    "\n",
    "boost.fit(X_train, y_train)\n",
    "boost_prediction = boost.predict(X_test)\n",
    "\n",
    "boost_classification_report = classification_report(y_test, boost_prediction, output_dict=True)\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = boost.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "print()\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_boost = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_boost, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = boost.predict(X_test) \n",
    "f1_boost = f1_score(y_test, predictions_test)\n",
    "roc_boost = roc_auc_score(y_test, predictions_test)\n",
    "print(\"F1 Score: {}\".format(f1_boost))\n",
    "print(f\"ROC: {roc_boost}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_boost = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_boost, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tunning <a class=\"anchor\" id=\"parameter-tunning-10\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_classifier = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "ada_grid_search = GridSearchCV(ada_classifier, scoring=\"roc_auc\", cv=3, param_grid={})\n",
    "ada_grid_search.fit(X_train, y_train)\n",
    "print('Best score: {}'.format(ada_grid_search.best_score_))\n",
    "\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TRAIN\")\n",
    "predictions_train = ada_grid_search.predict(X_train)\n",
    "print(\"F1 Score: {}\".format(f1_score(y_train, predictions_train)))\n",
    "print(f\"ROC: {roc_auc_score(y_train, predictions_train)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_train, predictions_train, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_ada = confusion_matrix(y_train, predictions_train)\n",
    "sb.heatmap(confusion_matrix_ada, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()\n",
    "\n",
    "print(53 * '=')\n",
    "print(\"TEST\")\n",
    "predictions_test = ada_grid_search.predict(X_test) \n",
    "print(\"F1 Score: {}\".format(f1_score(y_test, predictions_test)))\n",
    "print(f\"ROC: {roc_auc_score(y_test, predictions_test)}\")\n",
    "print(\"Classification Report: \")\n",
    "print(classification_report(y_test, predictions_test, target_names=['not pay', 'pay']))\n",
    "sb.set(font_scale=1.0)\n",
    "ax = plt.subplot()\n",
    "confusion_matrix_ada = confusion_matrix(y_test, predictions_test)\n",
    "sb.heatmap(confusion_matrix_ada, annot=True, ax=ax, fmt=\"g\")\n",
    "ax.set_xlabel('Predicted Grades');\n",
    "ax.set_ylabel('Observed Grades');\n",
    "ax.set_title('Confusion Matrix');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Voting <a class=\"anchor\" id=\"voting\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vote = VotingClassifier(\n",
    "#      estimators=[('dt', DecisionTreeClassifier()), ('svm', LinearSVC()), ('xgb', XGBClassifier())],\n",
    "#      voting='hard', weights=[1,1,1]\n",
    "# )\n",
    "\n",
    "# vote.fit(X_train, y_train)\n",
    "# vote_prediction = vote.predict(X_test)\n",
    "\n",
    "# vote_classification_report = classification_report(y_test, vote_prediction, output_dict=True)\n",
    "\n",
    "# print(f\"Classification report:\\n{classification_report(y_test, vote_prediction, labels=np.unique(y_train))}\\n\")\n",
    "\n",
    "# sb.set(font_scale=1.0)\n",
    "\n",
    "# ax = plt.subplot()\n",
    "\n",
    "# confusion_matrix_vote = confusion_matrix(y_test, vote_prediction)\n",
    "\n",
    "# sb.heatmap(confusion_matrix_vote, annot=True, ax=ax, fmt=\"g\")\n",
    "\n",
    "# ax.set_xlabel('Predicted Grades');\n",
    "# ax.set_ylabel('Observed Grades');\n",
    "# ax.set_title('Confusion Matrix');\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table = go.Figure(data=[go.Table(header=dict(values=['Algorithm', 'Accuracy Normal']),\n",
    "#                 cells=dict(values=[['DT', 'KNN', 'SVM', 'NN', 'LR', 'NB', 'RF', 'XGB', 'MLP', 'BOOST', 'VOTE'], [dtc_classification_report['accuracy'], knn_classification_report['accuracy'], svm_classification_report['accuracy'], nn_classification_report['accuracy'], lr_classification_report['accuracy'], nb_classification_report['accuracy'], rf_classification_report['accuracy'], xgb_classification_report['accuracy'], mlp_classification_report['accuracy'], boost_classification_report['accuracy'], vote_classification_report['accuracy']]]))\n",
    "#                     ])\n",
    "\n",
    "table = go.Figure(data=[go.Table(header=dict(values=['Algorithm', 'Accuracy Normal', 'F1 Score Normal', 'ROC Score Normal']),\n",
    "                 cells=dict(values=[['DT', 'KNN', 'SVM', 'NN', 'LR', 'NB', 'RF', 'MLP', 'BOOST'], \n",
    "                                    [dtc_classification_report['accuracy'], knn_classification_report['accuracy'], svm_classification_report['accuracy'], nn_classification_report['accuracy'], lr_classification_report['accuracy'], nb_classification_report['accuracy'], rf_classification_report['accuracy'], mlp_classification_report['accuracy'], boost_classification_report['accuracy']], \n",
    "                                   [f1_dt, f1_knn, f1_svm, f1_nn, f1_lr, f1_nb, f1_rf, f1_mlp, f1_boost], [roc_dt, roc_knn, roc_svm, roc_nn, roc_lr, roc_nb, roc_rf, roc_mlp, roc_boost]]))\n",
    "                     ])\n",
    "\n",
    "table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_no_ids.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [back](#index)\n",
    "## Apply Model <a class=\"anchor\" id=\"apply-model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = test_data_no_ids.drop(columns=['status', 'loan_id'])\n",
    "\n",
    "pred_comp = boost.predict_proba(test_inputs)\n",
    "\n",
    "pred_comp = pd.DataFrame(pred_comp, columns=['col2', 'Predicted'])\n",
    "\n",
    "pred_comp.drop('col2', axis=1, inplace=True)\n",
    "all_ids_comp = pd.DataFrame(all_ids_comp, columns=['Id'])\n",
    "results = pd.concat([all_ids_comp, pred_comp], axis=1)\n",
    "#results = results.rename(columns={\"loan_id\":\"Id\"})\n",
    "results.to_csv('results.csv', index = False)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
